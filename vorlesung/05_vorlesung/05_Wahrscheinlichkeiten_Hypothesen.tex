
\section{Konzept der statistischen Erwartung}

Das \textsl{Signifikanzniveau} $\alpha$ und die dazu komplementäre Wahrscheinlichkeit,
das \textsl{Vertrauensniveau} $1 - \alpha$ spielen eine zentrale Rolle bei der Bewertung
von Messergebnissen. Als wir in Kapitel~\ref{KonzepteinverseProbleme} die Konzepte der statistischen Auswertung
behandelt hatten,
haben wir diese beiden Begriffe gemeinsam mit dem des \textsl{Vertrauensintervalls} bzw.\
\textsl{Credible Interval} eingeführt, mit dem gemeinsamen Oberbegriff \textsl{Überdeckungsintervall}.
Es ist das Intervall, in dem mit einer Wahrscheinlichkeit von $P(X_1) = 1 - \alpha$ die
beobachteten Werte $X_{1,j}$ zur Größe $X_1$ liegen, das also die \glqq meisten\grqq
~Beobachtungen abdeckt.

Die Idee hinter dem Begriff des \textsl{Vertrauensintervalls} oder des \textsl{Credible Interval}s
ist, dass beim Bestimmen einer physikalischen Größe $X$ eine Vorstellung (eine Erwartung) darüber
existiert, wie die gemessenen oder beobachteten Werte verteilt sind. Das schließt die
Erwartung von Position und Breite des Bereichs, in dem die Werte liegen sollten, ein.
Diese Erwartung wird quantifiziert mit einer Angabe über die Wahrscheinlichkeit, dass die
zu messende Größe diesen oder jenen Wert annimmt, eher noch in welchem Bereich (Intervall) der
Wert zu erwarten ist. Wir \textsl{vertrauen} also darauf, dass wir einen Wert messen, der innerhalb eines
gewissen Intervalls liegen müsste, oder sagen, dass es plausibel oder \textsl{glaubwürdig}
(engl.\ \textsl{credible}) ist, dass die Beobachtungen in dem Intervall liegen.
Statt zu sagen, dass wir \textsl{eine Vorstellung haben}, können wir auch sagen,
dass wir \textsl{eine Hypothese aufstellen}.

Die Normalverteilung (Gaußverteilung) ist die grundlegende Verteilungsdichtefunktion
zur Beschreibung zufälliger Prozesse, beispielsweise Zerfallsprozesse und Diffusionsprozesse.
Bei rein stochastischen Prozessen wird von Zufallsgrößen ausgegangen, deren
Wahrscheinlichkeitsdichteverteilung eine Normalverteilung ist
\begin{equation}
p(X) \; = \; \frac{1}{\sqrt{2 \pi} \, \sigma} \, e^{-\frac{1}{2}\left(\frac{X - \mu}{\sigma}\right)^2}.
\end{equation}
Verteilungen wie die Poisson-Verteilung und die Binominal-Verteilung gehen für
unendlich viele Beobachtungen oder Stichprobenentnahmen in die Gaußverteilung über.

In der Wahrscheinlichkeitstheorie wird diese Tatsache exakt formuliert und ist der
\textsl{zentrale Grenzwertsatz} (engl.\ \textsl{central limit theorem}) CLT.
In einfacheren Worten können wir sagen:
\begin{quote}
Die Summe einer großen Anzahl von unabhängigen Zufallsgrößen folgt
asymptotisch einer stabilen Verteilung.
\end{quote}
Bei endlicher und positiver Varianz der Zufallsgrößen ist ihre Summe annähernd
normalverteilt, was die Sonderstellung der Normalverteilung erklärt.

Hierzu wird das Ziehen eines Wertes (das Machen einer Beobachtung / das Eintreten eines
Ereignisses) als Zufallsgröße interpretiert.
Damit betrachten wir eine Stichprobe als Folge von Zufallsgrößen. Sind die Elemente
einer Stichprobe Zufallsgrößen, die
\textsl{unabhängig und identisch verteilt} (Abkürzung: u.i.v.) sind, so ist auch ihre Summe
eine Zufallsgröße, die normalverteilt ist, wenn der Stichprobenumfang beliebig groß ist,
also gegen Unendlich konvergiert. In der englischsprachigen Literatur ist
\textsl{unabhängig und identisch verteilt} unter dem Begriff
\textsl{independent and identically distributed}, kurz i.i.d., zu finden.

Liegt nun eine Einzelbeobachtung innerhalb der flachen
Ausläufer (\textsl{tails}) der Wahrscheinlichkeitsdichteverteilung,
also außerhalb des Vertrauensintervalls, so
spricht man dann davon, dass der Wert (die Einzelbeobachtung)
signifikant von dem, was zu erwarten ist, abweicht. Das heißt, dass ein Ereignis eintritt,
das \textsl{nicht der Erwartung entspricht}. Somit \textsl{vermuten wir ein Problem}. Das Aufspüren
des Problems erfolgt durch \textsl{Hypothesentests}. Da die Normalverteilung
lange Ausläufer hat, gibt es eine kleine Wahrscheinlichkeit $\alpha$ dafür, dass es
weiter abseits liegende Werte gibt. Deshalb gilt es zu untersuchen,
ob solch ein Wert in der Tat im Rahmen der Normalverteilung weiter weg liegt
oder aber ob er auf Basis eines anderen Effektes gewonnen
wurde, ob er - statistisch formuliert - zu einer anderen Grundgesamtheit gehören könnte.

Der \textsl{Kolmogorow-Smirnow-Test} ist ein möglicher Test zum Prüfen, ob eine aus Beobachtungen
empirisch ermittelte Verteilung zu einer für die statistische Analyse der Beobachtungen
zugrundegelegten (erwarteten) Wahrscheinlichkeitsdichtefunktion passt.

% 30. Okt 2017

\section{Kolmogorow-Smirnow-Test}

\begin{figure}
\begin{center}
\includegraphics[width=80mm]{05_vorlesung/media/learn_robust.pdf}
\includegraphics[width=80mm]{05_vorlesung/media/learn_robust_2.pdf}
\caption{Beispiel für nicht erwartungsgemäße Beobachtungen (\textsl{rechts}) und
Umgewichten um die Verteilungform der Normalverteilung anzupassen \textsl{links}}
\label{biasExample}
\end{center}
\end{figure}
Das in Abb.~\ref{biasExample} dargestellte Beispiel zeigt dieselben Diagramme und
Daten, die wir in Abschnitt~\ref{robustEstimation} bereits zur Veranschaulichung
robuster Schätzverfahren behandelt hatten. Den robusten Schätzverfahren liegt die
im folgenden erläuterte statistische Betrachtung zugrunde: Es liegt eine Annahme
(Hypothese) darüber vor, welcher Gestalt die Wahrscheinlichkeitsverteilung der
Beobachtungen ist und ob eine Reihe von Beobachtungen zu einer gemeinsamen Verteilung
gehören oder zu unterschiedlichen Verteilungen. Technisch gesehen stellen sich die
Fragen, die statistisch formuliert lauten \glqq gehören Beobachtungen zu einer
Verteilung\grqq ~oder \glqq gehören Beobachtungen zu mehreren verschiedenen Verteilungen\grqq,
als
\begin{itemize}
\item \glqq stammen die Messwerte aus einem Prozess\grqq ~oder \glqq aus unterschiedlichen
Prozessen\grqq,
\item \glqq werden alle Messwerte von demselben Effekt beeinflusst \grqq ~oder \glqq gibt es
für verschiedene Gruppen aus der Messreihe unterschiedliche beeinflussende Effekte\grqq.
\end{itemize}
Die Annahmen bzw.\ Hypothesen bezüglich der Verteilungen zur Wahrscheinlichkeit
von Beobachtungswerten werden geprüft. Diese Art der Prüfung wird \textsl{Hypothesentest}
genannt. Es gibt unterschiedliche Typen von Hypothesentests, einer davon ist
der \textsl{Kolmogorow-Smirnow-Test}.
Er ist ein statistischer Test auf Übereinstimmung zweier Wahrscheinlichkeitsverteilungen.

Der Test verwendet die kumulierten Wahrscheinlichkeitsfunktionen
\begin{equation}
P(X) \; = \; \int\limits_{-\infty}^X \, p(X^\prime) \, \mathrm{d} X^\prime .
\end{equation}
Das hat den Vorteil, dass
die Verteilung der empirischen Häufigkeiten ohne Histogrammierung, d.h.\ ohne Einteilung in Klassen,
erfolgen kann.

Wir wollen damit prüfen, ob die Beobachtungen des in Abschnitt~\ref{robustEstimation} zur robusten
Schätzung betrachteten Beispiels der Annahme folgt, zu der Grundgesamtheit \textsl{genau einer}
Zufallsgröße $X_1$ zu gehören. Ferner nehmen wir an, dass ihre Ausprägungen (Beobachtungswerte)
unabhängig und identisch der Normalverteilung folgen.
\begin{quote}
Die Nullhypothese lautet:\\
Die Beobachtungen zu $X_1$ gehören zu einer
normalverteilten Grundgesamtheit.
\end{quote}
Wir prüfen, ob die Stichprobe $(X_{1,1}, X_{1,2}, \dots, X_{1,J})$ bezüglich
des Mittelwertes $y \, = \, \frac{1}{J} \sum_{j=1}^J X_{1,j}$ und der
empirischen Varianz $s^2 \, = \, \frac{1}{J-1} \sum_{j=1}^J (X_{1,j} \, - \, y)^2$ normalverteilt
ist. Die kumulierte Normalverteilung, mit der wir die kumulierte Verteilung der relativen Häufigkeiten
vergleichen, ist
\begin{equation}
P(X) \; = \;  \frac{1}{\sqrt{2\pi} s}
\int\limits_{-\infty}^X \, e^{-\frac{1}{2}\left(\frac{X^\prime \, - \, y}{s}\right)^2} \, \mathrm{d} X^\prime.
\label{cdfKS}
\end{equation}
Die kumulierte Gaußverteilung entspricht im wesentlichen der
Fehlerfunktion $\mathrm{erf}$, englisch \textsl{Error Function}.
\begin{equation}
\mathrm{erf}(X) \; := \; \frac{2}{\sqrt{\pi}} \; \int\limits_0^X \, e^{-X^{\prime 2}} \, \mathrm{d} X^\prime.
\label{erf}
\end{equation}
Die unterschiedlichen Definitionen für die kumulierte Gaußverteilung
und die Fehlerfuntion führen zu unterschiedlichen Wertebereichen.
Während die kumulierte Gaußverteilung einen Wertebereich von $0$ bis $1$ hat, ist die Fehlerfunktion so
definiert, dass sie einen Wertebereich von $-1$ bis $1$ hat. Sie lassen sich wie folgt in einander
überführen
\begin{equation}
P(X) \; = \; \frac{1}{2} \left(1 \, + \, \mathrm{erf}(\frac{X - y}{\sqrt{2} \, s})\right).
\end{equation}
Für standardnormalverteilte Zufallszahlen $Z = \frac{X-\mu}{\sigma}$ oder $Z = \frac{X-y}{s}$
wird für die kumulierte Gaußverteilung der Bezeichner $\Phi$ verwendet
\begin{equation}
 \Phi(Z) := \frac{1}{\sqrt{2\pi}}
\int\limits_{-\infty}^Z \, e^{-\frac{1}{2} Z^{\prime 2} } \, \mathrm{d} Z^\prime
\quad \Rightarrow \quad
\Phi(Z) \; = \; \frac{1}{2} \left(1 \, + \, \mathrm{erf}(\frac{Z}{\sqrt{2}})\right).
\label{cdfPhi}
\end{equation}

Die Kenntnis dieser unterschiedlichen Definitionen und wie sie ineinander umzurechnen sind ist
wichtig, weil die statistischen Bibliotheksfunktionen von Matlab, Gnu-Octave, Python, etc.\
nicht die kumulierte Gaußverteilung bzw.\ die kumulierte Standardnormalverteilung, sondern die Fehlerfunktion
zur Verfügungs stellen.

Die kumulierte Verteilung der relativen Häufigkeiten gewinnen wir darüber, dass wir die Werte der
Stichprobe in aufsteigender Reihenfolge sortieren
\begin{equation}
X_{1,k_1} \, \leq \, X_{1,k_2} \, \leq \,  \dots \, \leq \,  X_{1,k_J}
\end{equation}
Die Teilstichprobe mit der Beobachtung mit dem kleinsten Wert $(X_{1,k_1})$ hat die
relative kumulierte Häufigkeit $\frac{1}{J}$, die Teilstichprobe mit den zwei
kleinsten Beobachtungen $(X_{1,k_1}, X_{1,k_2})$ hat die relative kumulierte Häufigkeit $\frac{2}{J}$.
Die Teilstichprobe mit den $j$ kleinsten Beobachtungen
\begin{equation}
(X_{1,k_1}, X_{1,k_2}, \dots, X_{1,k_j})
\quad \text{hat~die~relative~kumulierte~Häufigkeit}
\quad H(X_{1,k_j}) \; = \; \frac{j}{J}
\label{cdfH}
\end{equation}
\begin{figure}
\begin{center}
\includegraphics[width=100mm]{05_vorlesung/media/KS_pdf_iteration0.pdf}
\caption{\label{cdf4KStest} Vergleich der Wahrscheinlichkeiten (kumulierte Verteilungsdichten) aus der
empirische gewonnenen Häufigkeit der beobachteten Werte der Zufallsgröße
(schwarze, durchgezogene Kurve) mit der kumulierten Normalverteilung, die sich auf
die empirischen Erwartungswerte, den Mittelwert und die empirische Varianz, bezieht (rote,
gestrichelte Kurve).}
\end{center}
\end{figure}

Abb.~\ref{cdf4KStest} stellt zum Vergleich die Wahrscheinlichkeitsfunktion
(kumulierte Verteilungsdichte) $P(X)$ gemäß Gl.~(\ref{cdfKS})
als rote, gestrichelte Kurve und die kumulierte, relative Häufigkeit
$H(X_{1,k_j}) \; = \; \frac{j}{J}$
als schwarze, durchgezogen gezeichnete Kurve in einem Diagramm dar.


Die Prüfgröße des \textsl{Kolmogorow-Smirnow}-Tests ist
\begin{equation}
\arraycolsep=2.4pt\def\arraystretch{2}
\begin{array}{l}
\sup\limits_{X} \mid H(X_{1,k_j}) \, - \, P(X) \mid \; = \;\\
\max\limits_{k_j} \left\{ \max \left\{ \mid H(X_{1,k_j}) \, - \, P(X_{1,k_j}) \mid, \;
\lim\limits_{X \rightarrow X_{1,k_j}} \mid H(X) \, - \, P(X_{1,k_j} - 1) \mid \right\} \; \right\} .
\end{array}
\end{equation}
Dabei steht sup für Supremum, was \textsl{kleinste obere Schranke} heißt.
Zu jeder Ausprägung der Stich\-probe wird der Betrag der Differenz der beiden
Wahrscheinlichkeiten berechnet. Die Berechnung des Grenzwertes
$\lim\limits_{X \rightarrow X_{1,k_j}} \mid H(X) \, - \, P(X_{1,k_j} - 1) \mid$
für das Supremum ist erforderlich, wenn die kumulierte relative Häufigkeit
(Summenhäufigkeit) durch Kumulieren eines Histogramms, also einer in Klassen
quantisierten Stichprobe, berechnet wurde. Dabei soll die Schreibweise mit
dem Grenzwert
$$
\lim\limits_{X \rightarrow X_{1,k_j}} \mid H(X) \, - \, P(X_{1,k_j} - 1) \mid
$$
zum Ausdruck bringen, dass man infinitesimal dicht aber doch prinzipiell rechts
von $X_{1,k_j}$ an der Stufe ist, praktisch wird aber direkt
$\mid H(X_{1,k_j}) \, - \, P(X_{1,k_j} - 1) \mid$
berechnet. Für die nichtklassierte, sortierte Stichprobe berechnen wir einfach nur
\begin{equation}
\max\limits_{k_j} \left\{ \mid H(X_{1,k_j}) \, - \, P(X_{1,k_j}) \mid \right\} .
\end{equation}

Die Wahrscheinlichkeit, dass die Beobachtungen nicht zur in der Hypothese angenommenen
Verteilung passt, ist die Wahrscheinlichkeit, dass die Beobachtung
in den Ausläufern (\textsl{Tails}) der Dichteverteilung liegt, weshalb wir
die durch die Fläche in den \textsl{Tails} repräsentierte Wahrscheinlichkeit mit
\textsl{Signifikanzniveau} $\alpha$ bezeichnen.

Beim Kolmogorow-Smirnow-Test wird die Nullhypothese verworfen, wenn
die empirische relative Häufigkeit einen Grenzwert überschreitet,
der vom Signifikanzniveau $\alpha$ und dem Stichprobenumfang $J$ abhängt
\begin{equation}
\sup\limits_{X} \mid H(X_{1,k_j}) \, - \, P(X) \mid \; > \; K_{\alpha, J}
\end{equation}
wobei es zu Schwellwerten $K_{\alpha, J}$ für $J < 35$ Tabellen in den gängigen
Formelsammlungen gibt und
für $J \geq 35$ folgende Näherung verwendet wird:
\begin{equation}
K_{\alpha, J} \; = \; \sqrt{-\frac{1}{2 \, J} \, \ln(\frac{\alpha}{2})}
\label{KSpruefgroesse}
\end{equation}


Hypothesentests in Python (\href{https://mybinder.org/v2/gh/dhueser/MDA-Vorlesung-iprom-tu-bs/master?urlpath=/lab/tree/vorlesung/05_vorlesung/code/hypothesentests.ipynb}{Klicke hier für interaktive Session}):
\lstinputlisting[style=Python]{05_vorlesung/code/example_KSTest.py}


\section{Wahrscheinlichkeitsdichtefunktionen und ihre Parameter}

Bei vielen beobachteten Stichproben, die signifikant von dem Modell, das besagt, dass sie zu genau einer
normalverteilten Grundgesamtheit gehören, abweichen, handelt
es sich um Beobachtungen aus Prozessen, denen mehrere überlagerte stochastische Anteile
zugrunde liegen. Bei dem in Abb.~\ref{biasExample} illustrierten Beispiel waren dies
zwei normalverteilte Grundgesamtheiten mit verschiedenen Erwartungswerten $\mu_1$ und
$\mu_2$ und mit unterschiedlichen Varianzen $\sigma^2_1$ und $\sigma^2_2$.

Für eine Bewertung von gemessenen Werten einer Größe, d.h.\ von Beobachtungen, werden statistische
Modelle zugrunde gelegt. Ein statistisches Modell zu beobachtbaren Größen umfasst die
Vorstellung von Zufallsgrößen und der Verteilung ihrer Beobachtungswerte.
Wahrscheinlichkeitsdichtefunktionen werden parametrisiert über ihre statistischen Momente.
Diese charakterisieren den Kurvenverlauf der Funktion hinsichtlich ihrer Lage, Breite,
Symmetrie und so weiter.

Der \textsl{Erwartungswert einer Zufallsgröße} $X$ ist das erste statistische Moment
der Verteilungsdichte $p$ ihrer Grundgesamtheit
\begin{equation}
\mathrm{E}(X) \; = \; \int\limits_{-\infty}^{\infty} \, X \, p(X) \, \mathrm{d} X \; =: \; \mu^{(1)}(X)
\end{equation}
das ihren Schwerpunkt angibt.
Der Erwartungswert der \textsl{Varianz}
einer Zufallsgröße $X$ entspricht dem zweiten statistischen Moment
der Verteilungsdichte $p$.
Das zweite statistische Moment ist definiert durch
\begin{equation}
\int\limits_{-\infty}^{\infty} \, X^2 \, p(X) \, \operatorname{d} X \; =: \; \mu^{(2)}(X)
\end{equation}
und die Varianz ist das zweite Moment der um den Erwartungswert verschobenen Zufallsgröße
\begin{equation}
\operatorname{Var}(X) \; = \; \int\limits_{-\infty}^{\infty} \, (X - \operatorname{E}(X))^2 \,
 p(X) \, \mathrm{d} X
\end{equation}
deren Wurzel
\begin{equation}
\sigma \; = \; \sqrt{\operatorname{Var}(X)}
\end{equation}
ein Maß für die Breite von $p$ darstellt.
Die Schiefe einer Verteilungsdichte wird durch das dritte statistische Moment
\begin{equation}
\int\limits_{-\infty}^{\infty} \, X^3 \, p(X) \, \mathrm{d} X \; =: \; \mu^{(3)}(X)
\end{equation}
gewonnen, bei dem die Zufallsgröße um ihren Erwartungswert verschoben und auf die Wurzel ihrer
Varianz normiert
\begin{equation}
\operatorname{Skew}(X) \; = \; \int\limits_{-\infty}^{\infty} \, \left(\frac{X - \mathrm{E}(X)}{\sigma}
\right)^3 \, p(X) \, \operatorname{d} X
\end{equation}
wird. Das dritte statistische Moment der normierten Zufallsgröße heißt
\textsl{Skewness}.

Ein Maß für die Überhöhung oder Abflachung relativ zur Normalverteilung wird durch
das vierte statistische Moment
\begin{equation}
\int\limits_{-\infty}^{\infty} \, X^4 \, p(X) \, \mathrm{d} X \; =: \; \mu^{(4)}(X)
\end{equation}
gewonnen, bei dem die Zufallsgröße um ihren Erwartungswert verschoben und auf die Wurzel ihrer
Varianz normiert
\begin{equation}
\operatorname{Kurt}(X) \; = \;  \int\limits_{-\infty}^{\infty} \, \left(\frac{X - \operatorname{E}(X)}{\sigma}
\right)^4 \, p(X) \, \operatorname{d} X
\end{equation}
wird. Das vierte statistische Moment der normierten Zufallsgröße heißt
\textsl{Kurtosis}.

% ======

%Die Kombination der Wahrscheinlichkeitsverteilungen von mehreren unabhängigen kontinuierlichen
%Zufallsgrößen $X_1, \dots, X_N$ haben wir in den vergangenen Vorlesungen bereits kennengelernt,
%als wir die Berechnung der Likelihood und der Posterior besprochen haben.
%Bei der Likelihood wurde jede Beobachtung als von jeder anderen Beobachtung unabhängige
%Zufallszahl mit gleichen Varianzen (identische Verteilungen) behandelt.
%Beim Posterior wurden die Größen, zu
%denen als {`a} priori Informationen ein Erwartungswert und eine Varianz gegeben sind,
%als jeweilige Zufallsgrößen behandelt. Mit Zufallsgrößen $X_1, \dots, X_N$ können
%auch verschiedene Messgrößen sein, wie beispielsweise Strom und Widerstand (hier $N=2$),
%oder drei Koordingaten im Raum ($N=3$), oder zwei Positionskoordinaten auf einem CCD-Chip
%gemeinsam mit dem jeweiligen Intensitätswert ($N=3$).

%Gegeben seien $N$ Zufallsgrößen $X_1, \dots, X_N$ und zu jeder Zufallsgröße
%sei eine Wahrscheinlichkeitsdichteverteilung $p(X_i)$ mit $i=1,\dots,N$ gegeben.
%Dabei sei $p$ eine beliebige Verteilung. Sie muss keine Normalverteilung sein.
%Die Zufallsgrößen seinen kontinuierlich, es gelte $X_i \in I \!\! R$ und für den Zufallsvektor
%$\mathrm{X} = (X_1, \dots, X_N) \in I \!\! R^n$.
%Dann gilt unter der Voraussetzung, dass sie unabhängig voneinander sind, für die gemeinsame Wahrscheinlichkeitsdichteverteilung $p(X_1, \dots, X_N)^\mathsf{T} = p(\mathbf{X})$:
%\begin{equation}
%p(\mathrm{X}) \; = \;
%\frac{1}{\int\limits_{-\infty}^{+\infty} \dots \int\limits_{-\infty}^{+\infty}
%\prod\limits_{i=1}^N p(X_i^\prime) \mathrm{d} X_1^\prime \dots \mathrm{d} X_N^\prime}
%\prod\limits_{i=1}^N p(X_i) .
%\end{equation}
%Wir wollen uns die Verteilung für zwei Zufallsgrößen, also $\mathbf{X}^\mathsf{T} = (X_1,X_2)$,
%für den Fall, dass beide Zufallgrößen normalverteilt seien genauer anschauen
%\begin{equation}
%p(\mathbf{X}) = \frac{1}{2 \pi \sigma_1 \sigma_2} \,
%e^{-\frac{1}{2} \,
%  \left(\frac{X_1-\mu_1}{\sigma_1}\right)^2
%  \, - \, \frac{1}{2} \,  \left(\frac{X_2-\mu_2}{\sigma_2}\right)^2}
%\end{equation}

% 6. Nov. 2017

\section{Vertrauensintervall und t-Verteilung}

Wir hatten bereits in Kapitel \ref{KonzepteinverseProbleme} angesprochen, wie
\textsl{Vertrauensintervall} $[x_1, x_2]$, Wahrscheinlichkeistdichtefunktion $p$,
\textsl{Signifikanzniveau} $\alpha$ und \textsl{Vertrauensniveau} $1-\alpha$ zusammen hängen.
Das Vertrauensintervall gibt den Bereich an, in dem ein Ereignis mit
Wahrscheinlichkeit $1-\alpha$ beobachtet wird bzw.\ die Messgröße einen Wert in diesem Bereich
annimmt, also
\begin{equation}
\int\limits_{x_1}^{x_2} p(X) \, \operatorname{d} X \; = \; 1 - \alpha .
\end{equation}
Eine Wahrscheinlichkeitsdichteverteilung $p(X)$
wird immer so normiert, dass ihre Fläche $1$ ist:
\begin{equation}
\int\limits_{-\infty}^\infty p(X) \, \operatorname{d} X \; = \; 1
\end{equation}
Das heißt mit anderen Worten:
Unsere Größe nimmt mit Wahrscheinlichkeit $1$ irgend einen beliebigen
Wert zwischen minus und plus Unendlich an.

Auch hatten wir in Kapitel \ref{KonzepteinverseProbleme} durchgenommen, wie man aus unterschiedlichen vorliegenden
Informationen eine Wahrscheinlichkeitsdichteverteilung berechnet, um den
Erwartungswert
\begin{equation}
E(X) \; = \; \int\limits_{-\infty}^\infty \, X  p(X) \, \operatorname{d} X
\end{equation}
zu bestimmen und um aus der Umkehrfunktion $P^{-1}$ der kumulativen Verteilung
das Über\-deck\-ungs\-intervall
\begin{equation}
[P^{-1}(\frac{\alpha}{2}), P^{-1}(1-\frac{\alpha}{2})]
\end{equation}
zu ermitteln.

Man kann zur Berechnung eines Überdeckungsintervalls
rechenzeiteffizient auf vorhandene tabellierte Werte für
die Umkehrfunktion der Normalverteilung zurück greifen, wenn
folgendes der Fall ist:
\begin{itemize}
\item Über die reinen Beobachtungen $X_{1,j}$ hinaus liegen keine
Informationen vor, also keine vorherigen Ergebnisse zu den zu
schätzenden Modellparameter (kein \textsl{Prior})
und keine weiteren beeinflussenden Verteilungen.
\item Es kann davon ausgegangen werden, dass die
einzelnen Beobachtungswerte um das Modell
normalverteilt streuen.
\end{itemize}
Für kleine
Stichprobenumfänge wird eine der Normalverteilung ähnliche Verteilung
herangezogen, die je kleiner der Stichprobenumfang ist, um so stärker ausgeprägte
Ausläufer (\textsl{Tails}) hat, die $t$-Verteilung. Diese werden wir im Laufe
dieses Kapitels beleuchten.

Zunächst erörtern wir die Berechnung der Vertrauensintervalle auf Basis
tabellierter Werte von Integrationsgrenzen zu ausgesuchten Ver\-trauens\-niveaus
der Normalverteilung. Eine solche Integrationsgrenze wird \textsl{Quantil}
genannt. Die Tabellen beziehen sich auf normierte Zufallsgrößen.
Für die Gaußverteilung normieren wir die Zufallsgröße, so dass der
Erwartungswert $\mu = 0$ ist und die Varianz $\sigma^2 = 1$, d.h.
$Z$ ist \textsl{standardnormalverteilt}
\begin{equation}
Z \; \sim \; \mathcal{N}(0,1) .
\label{standardNormalverteilt}
\end{equation}
Die Wahrscheinlichkeitsdichtefunktion $p(Z)$ ist die \textsl{Standard}normalverteilung
\begin{equation}
p(Z) \; = \; \frac{1}{\sqrt{2 \pi}} \, e^{-\frac{1}{2} Z^2}
\qquad \textrm{mit} \qquad Z \; = \; \frac{X - \mu}{\sigma} .
\end{equation}

Die in den Formelsammlungen und Tabellenwerken aufgelisteten Grenzen sind
als obere Integrationsgrenze definiert
\begin{equation}
\int\limits_{-\infty}^{z_\alpha} p(Z) \mathrm{d} Z \; = \; \alpha .
\label{WahrscheinlichkeitQuantil}
\end{equation}
In Abb.~\ref{normVertQuantil} wird dies anhand des Quantils $z_\alpha = 0.5244$
zu $\alpha = 0.7$ also $70 \%$ Wahrscheinlichkeit beispielhaft gezeigt.

\begin{figure}
\begin{center}
\includegraphics[width=79mm]{05_vorlesung/media/normpdfQuantil.pdf}
\hspace{5mm}
\includegraphics[width=81mm]{05_vorlesung/media/normcdfQuantil.pdf}
\caption{\label{normVertQuantil} Quantil $z_\alpha = 0.5244$ und Wahrscheinlichkeit
$\alpha = 0.7$ der Standard\-normalverteilung.}
\end{center}
\end{figure}

Dies lesen wir so:
\begin{quote}
Die Ereignisse Werte zu $Z$ zu beobachten, die kleiner sind als $z_\alpha$, können
mit einer Wahrscheinlichkeit von $\alpha$ eintreten.
\end{quote}
Die in den Tabellenwerken gelisteten Wahrscheinlichkeiten $\alpha$ werden
aus der Integration der Wahrscheinlichkeitsdichte der normierten Zufallsgröße
von minus Unendlich bis zu einer endlichen oberen Integrationsgrenze $z_\alpha$
gewonnen.
Die zu einem Wert eines Quantils gehörende Wahrscheinlichkeit wird durch den tiefgestellten
Index bei $z_\alpha$ gekennzeichnet, in unserem Beispiel aus Abb.~\ref{normVertQuantil}
ist dies dann
$$
z_{0.7} \; = \; 0.5244
$$
Als weiteres Beispiel betrachten wir die Frage, wie hoch die Wahrscheinlichkeit ist,
Werte zu beobachten, die in den Ausläufern einer Verteilungsdichte liegen.
Die Ausläufer werden im Englischen
als \textsl{Tails} (Schwänze) bezeichnet. Dies sollen Beobachtungen zu der normierten
Zufallsgröße $Z$ mit Werten, die zum einen kleiner oder gleich $-1.960$ sind, sein
$$
z_{0.025} \; = \; -1.960
$$
und zum anderen, die Beobachtungen mit Werten größer oder gleich $1.960$, also die
in dem rechten Ausläufer (\textsl{Tail}) der Standardnormalverteilung liegen.
Die Wahrscheinlichkeit für Beobachtungen im linken \textsl{Tail} ist
\begin{equation*}
\int\limits_{-\infty}^{-1.960} p(Z) \mathrm{d} Z \; = \; 0.025 .
\end{equation*}

Aufgrund der Symmetrie der Standardnormalverteilung ist die Wahrscheinlichkeit für das Liegen von
Werten im rechten \textsl{Tail} ebenfalls $0.025$. Wir rechnen also zusammen, dass mit
einer Wahrscheinlichkeit von $0.05 = 0.025+0.025$ Werte in den beiden \textsl{Tails} außerhalb
des Intervalls $[-1.960, 1.960]$ liegen können. Somit ist die Wahrscheinlichkeit, dass sie
innerhalb des Intervalls liegen $1 - 0.05 = 0.95$
\begin{equation}
\int\limits_{-1.96}^{1.96} \, p(Z) \, \operatorname{d} Z \; = \; 0.95
\end{equation}
was uns zeigt, wie wir die Tabellen zu verwenden haben.
Wenn wir ein zweiseitiges Intervall betrachten wollen, wie hier zu einem Vertrauensniveau
von $95 \%$, so müssen wir im Tabellenwerk nach dem Quantil, das für $97.5 \%$ gelistet
ist, schauen
$$
z_{0.975} \; = \; 1.960
$$
weil
$$
0.95 \; = \;
\underbrace{\int\limits_{-\infty}^{z_{0.975}} \, p(Z) \, \operatorname{d} Z}_{0.975} \; - \;
\underbrace{\int\limits_{-\infty}^{z_{0.025}} \, p(Z) \, \operatorname{d} Z}_{0.025}
$$
Aufgrund der Achssymmetrie der Normalverteilung gilt
$$
z_{\frac{1}{2}\alpha} = -z_{1-\frac{1}{2}\alpha} .
$$
Da sich die Quantile zum jeweiligen Vertrauensniveau $1-\alpha$ auf die normierte Zufallsgröße
beziehen, muss für die Berechnung des Vertrauensintervalls zurückgerechnet werden
\begin{equation}
Z \; = \; \frac{X - \mu}{\sigma} \qquad \Leftrightarrow \qquad X \; = \; \mu + Z \sigma
\end{equation}
also
\begin{equation}
[z_{\frac{1}{2}\alpha}, z_{1-\frac{1}{2}\alpha}] =
[-z_{1-\frac{1}{2}\alpha}, z_{1-\frac{1}{2}\alpha}]
 \qquad \Leftrightarrow  \qquad
[\mu \, - \, z_{1-\frac{1}{2}\alpha} \, \sigma, \mu \, + \, z_{1-\frac{1}{2}\alpha} \, \sigma]
\end{equation}
Mit $\alpha$ sind die $5 \%$ Signifikanzniveau und mit $1-\alpha$ die $95 \%$ Vertrauensniveau gemeint.

Die Likelihood wird maximal für
\begin{equation}
y \; = \; \frac{1}{J} \sum_{j=1}^J X_{1,j} ,
\end{equation}
wobei $y$ der Schätzwert für den Modellparameter $Y$ ist.
Der Schätzwert $s^2$ für die Varianz $\sigma^2$ wird berechnet mit
\begin{equation}
s^2 \; = \; \frac{1}{J-1} \sum_{j=1}^J (X_{1,j} - y)^2 .
\end{equation}

Nachdem wir gesehen haben, wie wir für eine normalverteilte Zufallsgröße
das Vertrauensintervall ermitteln können, wenden wir uns jetzt der Thematik zu,
was passiert, wenn wir zu einer Zufallsgröße nur relativ wenige Beobachtungen haben.

%------------------- kleine Stichprobenumfaenge -> Motivation t-Verteilung
\begin{figure}
\begin{center}
\includegraphics[width=100mm]{05_vorlesung/media/learn_Student_t1samples.pdf}
\caption{\label{kumulWahrsch} Kumulative relative Wahrscheinlichkeitsfunktionen und relative
Summenhäufigkeiten von verschiedenen Stichproben mit jeweils $J \, = \ 50$ Werten, entnommen aus einer
Grundgesamtheit mit $\mu \, = \, 42$ und $\sigma \, = \, 5$.}
\end{center}
\end{figure}
Wenn wir aus einer sehr großen Stichprobe, die einer Grundgesamtheit entnommen wurde,
disjunkte Teilstichproben von kleinerem Umfang ($J < 100$) in Grüppchen unterteilen, so können wir
beobachten, dass sowohl die Mittelwerte streuen als auch die Varianzen.
Diese Grüppchen oder Teilstichproben können wir als mehrere Stichproben aus einer normalverteilten
Grundgesamtheit ansehen.
Abb.~\ref{kumulWahrsch} illustriert wie unterschiedlich die Verteilungsfunktionen von
Stichproben mit relativ kleinem Umfang ($J \, = \ 50$) aussehen können, obwohl sie zu derselben
Grundgesamtheit gehören. Dieses Beispiel wurde wie folgt mit Gnu-octave generiert:

\lstinputlisting[style=Matlab]{05_vorlesung/code/example_t.m}

Zur Bewertung der Flachheit bzw.\ Ausgeprägtheit der Ausläufer einer Verteilung berechnen wir
ihre \textsl{Kurtosis}. Die \textsl{Kurtosis} der Normalverteilung hat den Wert $3$.

Die empirischen Schätzer für den Erwartungswert, die Varianz, die Skewness und Kurtosis
berechnen wir aus einer Stichprobe mit Beobachtungswerten $X_{1,1}, \dots, X_{1,J}$
wie folgt:
\begin{itemize}
\item[] Der Mittelwert wird berechnet mit
\begin{equation}
y \; = \; \frac{1}{J}\sum_{j=1}^J \, X_{1,j} .
\end{equation}
\item[] Die empirische Standardabweichung wird berechnet mit
\begin{equation}
s \; = \; \sqrt{ \frac{1}{J-1}\sum_{j=1}^J \, (X_{1,j} - y)^2 }.
\label{empirischeStd}
\end{equation}
Zu bemerken ist, dass jetzt durch $J-1$ und nicht durch $J$ geteilt wird, weil
das $y$ aus den Werten der Stichprobe $X_{1,1}, \dots, X_{1,J}$ gewonnen wurde und somit
die Anzahl der Freiheitsgrade $\nu$ um Eins verringert wird, also $\nu = J-1$.
\item[]
Die empirische Skewness wird berechnet mit
\begin{equation}
\textrm{skew} \; = \; \frac{1}{J} \sum_{j=1}^J \, \left( \frac{X_{1,j} - y}{s} \right)^3 .
\end{equation}
\item[] Die empirische Kurtosis wird berechnet mit
\begin{equation}
\textrm{kurt} \; = \; \frac{1}{J} \sum_{j=1}^J \, \left( \frac{X_{1,j} - y}{s} \right)^4 .
\end{equation}
\end{itemize}
Bei dem in Abb.~\ref{kumulWahrsch} gezeigten Beispiel mit mehreren Stichprobenentnahmen aus
einer Grundgesamtheit erhalten wir folgende aus den Momenten abgeleiteten empirischen
Parameter der Verteilungen:
\begin{center}
Tabelle 1:

\begin{tabular}{c||c|c|c|c}
\hline
\multicolumn{5}{c}{Einzelstichproben, Umfang jeweils $J = 50$}\\
\hline
lfd.\ Nr. & $y$ & $s$ & skew & kurt\\
\hline\hline
1 & 40.79 &  4.83 &  0.33 &  2.50 \\
2 & 43.08 &  5.52 &  0.16 &  2.16 \\
3 & 40.92 &  4.38 & -0.26 &  2.57 \\
4 & 41.64 &  4.46 &  0.09 &  2.44 \\
5 & 41.41 &  4.98 &  0.59 &  4.16 \\
\hline\hline
\multicolumn{5}{c}{alle Stichproben zusammen, Umfang $J = 5 \cdot 50 = 250$}\\
\hline
 & 41.57 &  4.88 &  0.28 &  2.89 \\
\hline\hline
\multicolumn{5}{c}{wahre Grundgesamtheit}\\
\hline
 & $\mu$ & $\sigma$ & Skew & Kurt\\
\hline\hline
Normalvert. & 42.00 &  5.00 &  0.00 &  3.00 \\
\hline
\end{tabular}
\end{center}
%--------------------- Ende der Motivation fuer t-Verteilung

%==========
%Bei einer Überlagerung unterschiedlicher Prozesse, wie dies beispielsweise
%bei exponetiell verlaufenden Zerfällen (aus der Biologie oder Strahlenphysik) der Fall ist, sind die
%Verteilungen asymmetrisch und können z.B.\ mit einer Poissonverteilung beschrieben werden.

Man kann der Beschreibung einer kleinen Stichprobe besser gerecht werden, wenn die
Verteilung flacher ist als eine Standardnormalverteilung. Das bedeutet, dass ihre Ausläufer
ausgeprägter sind.
Eine Verteilungsdichtefunktion, die der Gaußverteilung ähnlich ist, aber unter
Berücksichtigung des begrenzten Stichprobenumfangs entsprechend ausgeprägtere Tails hat,
ist die im folgenden dargestellte \textsl{Student-t}-Verteilung, kurz auch
einfach $t$-Verteilung genannt.

%==================

%-----------------
Diese Wahrscheinlichkeitsdichteverteilung wurde anfang des 20. Jahrhunderts von
William Sealy Gosset entwickelt. Sie wird der
Tatsache gerecht, dass die Varianz zunimmt mit kleiner werdendem Stichprobenumfang.
Gosset veröffentlichte 1908 erstmals zu dem Thema während er als Chemiker für die Guinness-Brauerei
in Dublin arbeitete. Er entwickelte einen Test zum Vergleich von Mittelwerten
einzelner Stichproben als eine billige Art und Weise, die Qualität des Stout
zu überwachen. Dieser Test wird entsprechend der zugrunde gelegten Wahrscheinlichkeitsdichtefunktion
$t$-Test genannt.
Da sein Arbeitgeber die Veröffentlichung nicht gestattete, veröffentlichte Gosset sie unter
dem Pseudonym Student. Die zugehörige Theorie wurde erst durch die
Arbeiten von R. A. Fisher belegt, der die Verteilung \textsl{Student}-Verteilung (engl.\
\textsl{Student's distribution}) nannte.

Die $t$-Verteilung bezieht sich auf eine standardnormalverteilte Zufallsgröße $Z$ und sieht wie folgt aus
\begin{equation}
p_\mathrm{t}(Z,\nu) \; = \;
{\frac {\Gamma \left({\frac {\nu+1}{2}}\right)}{{\sqrt {\nu \pi }} \;
\Gamma \left({\frac {\nu}{2}}\right)}} \, \left(1+{\frac {Z^2}{\nu}}\right)^{-{\frac {\nu+1}{2}}} ,
\end{equation}
wobei $\nu$ die Anzahl der Freiheitsgrade ist (was für den Fall, dass $Z$
die Summe der Stichprobenelemente darstellt, um den Mittelwert der Stichprobe mit Umfang $J$
zu repräsentieren, dann $\nu = J-1$ ist), und wobei
die Gamma-Funktion für natürliche Zahlen $\nu \, \in I \!\! N$ dividiert durch Zwei
wie folgt definiert ist:
\begin{equation}
\Gamma \left({\frac{\nu}{2}}\right) \; = \;
\sqrt{\pi} \, \frac{(\nu-2)!!}{ 2^{\frac{\nu-1}{2}} } \qquad \nu \, \in I \!\! N
\label{GammaHalfInt}
\end{equation}
mit $\nu!! = \nu (\nu-2) (\nu-4) \dots 4 \cdot 2$.
Wir werden auf den theoretischen Hintergrund aus der Wahrscheinlichkeitstheorie nicht
genau eingehen, also ist die Definition dieser Verteilung auch kein Klausurstoff.
Für die Klausur werden die dazugehörigen Quantiltabellen zur Verfügung gestellt, d.h.\
den Aufgabenstellungen beigefügt.

Je kleiner der Stichprobenumfang ist desto flacher wird die Verteilung, mit entsprechend
längeren Ausläufern, \textsl{Tails}, siehe
Abb.~\ref{studentt}.
\begin{figure}
\begin{center}
\includegraphics[width=90mm]{05_vorlesung/media/learn_Student_tpdf.pdf}
\caption{\label{studentt} \textsl{Student-t}-Verteilungen für unterschiedliche Stichprobenumfänge.}
\end{center}
\end{figure}
Um zu verdeutlichen, wie sich die Gaußverteilung und die \textsl{Student-t}-Verteilung hinsichtlich
Breite und lange Ausläufer unterscheiden, haben wir hier dies im Vergleich berechnet.
Dabei haben wir bewusst ganz extrem wenig Freiheitsgrade gewählt, nämlich nur $5$.
Wir verwenden jetzt mal den empirischen Mittelwert und die empirische Standardabweichung,
die wir aus der Gesamtheit aller Stichproben gewonnen hatten, also $y = 41.57$
und $s = 4.88$. Daraus rechnen wir sowohl die Normalverteilung als auch die
\textsl{Student-t}-Verteilung für nur fünf Freiheitsgrade und erhalten eine größere Standardabweichung
und eine größere Kurtosis aus der \textsl{Student-t}-Verteilung:
\begin{center}
\begin{tabular}{l||c|c}
\hline
Verteilung & $\sigma$ & Kurt\\
\hline
Gauß &   4.88 &  2.99 \\
Student-t &  5.86 &  3.59 \\
\hline
\end{tabular}
\end{center}
Dementsprechend sind auch die Quantile größer.


Die Kurtosis der \textsl{Student-t}-Verteilungen nimmt Werte an, die um so größer werden, je flacher die Verteilungsdichte
ist, also je ausgeprägter die \textsl{Tails} sind, also je kleiner die Stichprobenumfänge bzw.\ die
Anzahl der Freiheitsgrade sind.

%----

Das Vertrauensintervall zu einem Vertrauensniveau von $1 - \alpha$ für eine endlich große
Strichprobe mit Stichprobenumfang $J$ schätzen wir mit dem Quantil der \textsl{Student-t}-Verteilung ab.
Die Anzahl der Freiheitsgrade entspricht dem Stichprobenumfang abzüglich der Anzahl der zu
schätzenden Modellparameter, was in diesem Fall einer ist, also $\nu = J - 1$.



%---------------

Während für die Standardnormalverteilung das Quantil $z_{0.975} \; = \; 1.96$ beträgt, haben
wir aus der \textsl{Student-t}-Verteilung zu demselben Vertrauensniveau für $\nu = 50$ Freiheitsgrade
$t_{0.975, 50} \; = \; 2.01$ und für $\nu = 5$ Freiheitsgrade $t_{0.975, 5} \; = \; 2.57$.

\begin{figure}
\begin{center}
\includegraphics[width=80mm]{05_vorlesung/media/vertrauensintervall_nu_11.pdf}
\caption{\label{Vertrauensintervalle} Vertrauensintervalle in Vergleich für die
Standardnormalverteilung (rote Kurve)
und die \textsl{Student-t}-Verteilung für $\nu = 11$ Freiheitsgrade (schwarze gestrichelte Kurve).}
\end{center}
\end{figure}
Für das zweiseitige, symmetrische Intervall verwenden wir das Quantil $t_{1-\alpha/2,\nu}$, so dass
wir folgendes Vertrauensintervall erhalten:
$$
[y - t_{1-\alpha/2,\nu} s, y + t_{1-\alpha/2,\nu} s]
$$
Die Breite wird durch die Anzahl der Freiheitgrade beeinflusst, je kleiner
die Anzahl der Freiheitsgrade ist, desto breiter wird das Intervall. Die beiden Abbn.~\ref{studentt}
und \ref{Vertrauensintervalle}
zeigen, dass die \textsl{Tails} einer \textsl{Student-t}-Verteilung für wenige Freiheitsgrade
deutlich breiter sind als bei der Standardnormalverteilung, folglich auch das
Vertrauensintervall breiter ist. In Abb.~\ref{Vertrauensintervalle} sind die Quantile für
ein Vertrauensniveau von $1 - \alpha = 0.95$ eingezeichnet.

Das \textsl{(vollständige) Messergebnis} geben wir an als
Schätzwert zur Größe $Y$ gemeinsam mit dem Vertrauensintervall.
Eine gebräuchliche Schreibweise ist
\begin{equation}
Y \; = \; y \, \pm \, t_{1-\alpha/2,\nu} s .
\label{vollstaendigesErgebKlassisch}
\end{equation}

Als nächstes befassen wir uns damit, das Vertrauensintervall des Mittelwertes abzuschätzen.
Dazu betrachten wir ein Beispiel, bei dem aus einer Grundgesamtheit mit den wahren
Werten $Y = 42$ und $\sigma = 7$ mehrere Stichproben genommen werden, beispielsweise $K = 15$,
wobei jede Stichprobe einen Umfang von $J = 25$ habe. Zu jeder der Stichproben $k = 1\dots K$
berechnen wir den Mittelwert $y_k$ und die empirische Standardabweichung $s_k$.

\begin{tabular}{c||c|c|c|c|c|c|c|c}
$k$   &  1     &    2   &    3   &   4    &    5   &   6    &   7    &   8   \\
\hline\hline
$y_k$ & 41.813 & 41.854 & 39.084 & 42.117 & 41.532 & 40.428 & 42.717 & 39.727\\
\hline
$s_k$ &  6.476 &  7.706 &  6.996 &  7.729 &  6.431 &  7.516 &  7.505 &  7.439\\
\end{tabular}

\vspace{3mm}

\begin{tabular}{c||c|c|c|c|c|c|c}
$k$   &  9     &    10  &   11   &   12   &   13   &   14   &   15  \\
\hline\hline
$y_k$ & 40.858 & 39.670 & 44.351 & 42.011 & 41.935 & 44.509 & 39.921\\
\hline
$s_k$ &  8.568 &  7.649 &  6.082 &  7.661 &  7.084 &  7.878 &  7.084\\
\end{tabular}

Der Mittelwert $\bar y$ der Mittelwerte liefert dann
$$
\bar y \; = \; \frac{1}{K} \sum\limits_{k=1}^K y_k \; = \; 41.502
$$
und die empirische Standardabweichung der Mittelwerte $y_k$ liefert
$$
\bar s \; = \; \sqrt{\frac{1}{K-1} \sum\limits_{k=1}^K (y_k \, - \, \bar y)^2 }
 \; = \; 1.606
$$
Das wiederholte Ziehen von Stichproben kann für so manche Anwendung kostspielig sein,
so dass es gilt die empirische Standardabweichung des Mittelwerts aus nur einer
Stichprobe abzuschätzen.
Dazu betrachten wir die Likelihood
\begin{equation}
l(Y, \sigma | \{X_{1,1}, \dots, X_{1,J}\}) \; = \;
\prod\limits_{j=1}^J \frac{1}{\sqrt{2 \pi} \, \sigma}
 e^{- \frac{1}{2} \, \left( \frac{(X_{1,j} - Y)}{\sigma} \right)^2 }
\label{LikelihoodY}
\end{equation}
und
setzten den Schätzer $y$ der Größe $Y$ als nahrhafte Null ein
\begin{equation}
l(Y, \sigma | \{X_{1,1}, \dots, X_{1,J}\}) \; = \;
\prod\limits_{j=1}^J \frac{1}{\sqrt{2 \pi} \, \sigma}
 e^{- \frac{1}{2} \, \left( \frac{(X_{1,j} - y) - (Y - y)}{\sigma} \right)^2 }
\label{LikelihoodNahrhafteNull}
\end{equation}
d.h.
\begin{equation}
l(Y, \sigma | \{X_{1,1}, \dots, X_{1,J}\}) \; = \;
 \frac{1}{(\sqrt{2 \pi} \, \sigma)^J}
 e^{- \frac{1}{2} \, \sum\limits_{j=1}^J \left( \frac{(X_{1,j} - y) - (Y - y)}{\sigma} \right)^2 } .
\end{equation}
mit folgender Nebenrechnung für die Summe im Exponenten
$$
\arraycolsep=2.4pt\def\arraystretch{2}
\begin{array}{ll}
\sum\limits_{j=1}^J \left( (X_{1,j} - y) - (Y - y) \right)^2  & =
\sum\limits_{j=1}^J \left( (X_{1,j} - y)^2 + (Y - y)^2 - 2 (X_{1,j} - y) (Y - y) \right) \\
 & = \left( \sum\limits_{j=1}^J  (Y - y)^2\right) +
 \left( \sum\limits_{j=1}^J  (X_{1,j} - y)^2\right) - 2 \sum\limits_{j=1}^J (X_{1,j} - y) (Y - y)\\
 & = J (Y - y)^2 +
 \left( \sum\limits_{j=1}^J  (X_{1,j} - y)^2\right) = J (Y - y)^2
\end{array}
$$
und $\sum\limits_{j=1}^J (X_{1,j} - y) (Y - y) = 0$ wegen $J y = \sum\limits_{j=1}^J X_{1,j}$
gilt für die Likelihood folgende Proportionalität
\begin{equation}
l(Y, \sigma | \{X_{1,1}, \dots, X_{1,J}\}) \; \propto \;
 e^{- \frac{1}{2} \, J \, \left( \frac{(Y - y)}{\sigma} \right)^2 }
\label{LikelihoodmitJ}
\end{equation}
und wir definieren folgende Größe
\begin{equation}
\bar \sigma^2 \; := \; \frac{\sigma^2}{J}
\label{VarianzMittelwert}
\end{equation}
als \textsl{Varianz des Mittelwertes}.

Damit lässt sich Gl.~(\ref{LikelihoodmitJ}) umschreiben in
\begin{equation}
l(Y, \sigma | \{X_{1,1}, \dots, X_{1,J}\}) \; \propto \;
 e^{- \frac{1}{2} \, \left( \frac{(Y - y)}{\bar \sigma} \right)^2 } .
\end{equation}
Nun setzen wir in die Definitionsgleichung
der Varianz des Mittelwertes Gl.~(\ref{VarianzMittelwert}) bzw.\
die Wurzel daraus die empirisch ermittelten Werte unseres Beispiels ein:
\begin{equation}
\bar s \; = \; \frac{s}{\sqrt{J}}
\label{empirischeVarianzMittelwert}
\end{equation}

\begin{tabular}{c||c|c|c|c|c|c|c|c}
$k$   &  1     &    2   &    3   &   4    &    5   &   6    &   7    &   8   \\
\hline\hline
$\bar s_k$ &  1.295 &  1.541 &  1.399 &  1.546 &  1.286 &  1.503 &  1.501 &  1.488\\
\end{tabular}

\vspace{3mm}

\begin{tabular}{c||c|c|c|c|c|c|c}
$k$   &  9     &    10  &   11   &   12   &   13   &   14   &   15  \\
\hline\hline
$\bar s_k$ &  1.714 &  1.530 &  1.216 &  1.532 &  1.417 &  1.576 &  1.417\\
\end{tabular}

Jede einzelne Stichprobe liefert eine etwas unterschiedliche Abschätzung für die
Standardabweichung des Mittelwertes. Die Werte für $\bar s$ liegen mit $1.3$ bis $1.5$
knapp unter $1.6$. Es wird die Standardabweichung des Mittelwertes leicht unterschätzt, weil
der einzelne Stichprobenumfang mit $J = 25$ größer ist als die Anzahl der Stichproben mit
$K = 15$.

Das \textsl{Vertrauensintervall für die Schätzung des Mittelwertes} im Gegensatz zur
Schätzung des Einzelwertes ist damit
$$
[y - t_{1-\alpha/2,\nu} \bar s, y + t_{1-\alpha/2,\nu} \bar s]
$$
und in der Schreibweise als \textsl{vollständiges Messergebnis}
\begin{equation}
Y \; = \; y \, \pm \, t_{1-\alpha/2,\nu} \bar s .
\label{vollstaendigesErgebMittelwert}
\end{equation}


%===============


\section{\textsl{t}-Test - Mittelwerttest}


Nicht nur um die Qualität von schwarzem Bier (Stout) zu überprüfen, sondern ganz
allgemein, wird der \textsl{t}-Test eingesetzt, um zu testen, ob
\begin{enumerate}
\item eine Stichprobe zu einer Grundgesamtheit gehört,
 deren Erwartungswert bekannt ist;
\item zwei Stichproben zur gleichen Grundgesamtheit gehören,
 deren Erwartungswert nicht bekannt ist, sondern die beiden
 Mittelwerte der jeweiligen Stichproben miteinander verglichen werden.
\end{enumerate}

Die Nullhypothese $H_0$ für den Test (1.), ob der Erwartungswert $\mu_1$ einer
Stichprobe zu einer Grundgesamtheit gehört
und damit dem Erwartungswert der Grundgesamtheit, deren Erwartungswert $\mu_0$ bekannt ist,
entspricht lautet: Der Erwartungswert $\mu_1$ der Stichprobe
ist gleich dem  Erwartungswert $\mu_0$ der Grundgesamtheit, $\mu_1 = \mu_0$.
Die Alternativhypothese $H_\mathrm{a}$
lautet, dass sie einen anderen Erwartungswert hat, also $\mu_1 \neq \mu_0$,
und damit zu einer anderen Grundgesamtheit gehört.

Der Test auf Vergleich des Erwartungswerts einer Stichprobe, der Einstichproben-\textsl{t}-Test,
wird wie folgt formuliert:
\begin{center}
\begin{tabular}{c|cl}
$H_0$ & $\mu_1 = \mu_0$ & die Stichprobe hat den gleichen Erwartungswert wie die Grundgesamtheit\\
\hline
$H_\mathrm{a}$ & $\mu_1 \neq \mu_0$ & die Stichprobe hat einen anderen Erwartungswert und\\
 & & gehört somit nicht zur Grundgesamtheit
\end{tabular}
\end{center}

Die Prüfgröße ist die Differenz zwischen empirisch berechnetem Mittelwert der Stichprobe
und Erwartungswert der Grundgesamtheit normiert auf die
Standardabweichung des Mittelwerts $\bar s$ der Stichprobe. Mit $y$ für den Mittelwert der
Stichprobe ist $y = \frac{1}{J}\sum_{j=1}^J X_{1,j}$ und mit der Varianz des Mittelwerts
\begin{equation}
\bar s^2 \; = \; \frac{1}{J (J - 1)} \sum_{j=1}^{J} \, (X_{1,j} - y)^2
\end{equation}
ist die Prüfgröße des Einstichproben-\textsl{t}-Tests wie folgt definiert
\begin{equation}
T \; = \; \frac{y \, - \, \mu_0}{\bar s}
\label{tTestonesample}
\end{equation}
Die Nullhypothese wird mit einem Signifikanzniveau von $\alpha = 0.05$ abgelehnt, wenn
der Betrag der Prüfgröße größer als das entsprechende Quantil der \textsl{t}-Verteilung ist
\begin{equation}
|T| \; > \; t_{1-\frac{1}{2} \alpha, \, \nu} \qquad \Rightarrow \qquad H_0 \; \; \mathrm{ablehnen}
\end{equation}
wobei $\nu$ die Anzahl der Freiheitsgrade der Stichprobe ist, $\nu = J-1$.

Der Test (2.), ob zwei Stichproben zur gleichen Grundgesamtheit gehören,
 deren Erwartungswert nicht bekannt ist, sieht wie folgt aus:
Sei $\mu_1$ der Erwartungswert der Grundgesamtheit zu Stichprobe 1 mit
Beobachtungen $(X_{1,1},\dots,$ $X_{1,J_1})$ und sei $\mu_2$ der Erwartungswert der
zu Stichprobe 2 mit Beobachtungen $(X_{2,1}, \dots, X_{2,J_2})$.
Die Nullhypothese $H_0$ lautet, dass beide denselben Erwartungswert haben,
was soviel bedeutet wie $\mu_1 = \mu_2$. Die Alternativhypothese $H_\mathrm{a}$
lautet, dass sie unterschiedliche Erwartungswerte haben, was soviel bedeutet wie $\mu_1 \neq \mu_2$,
und damit zu unterschiedlichen Grundgesamtheiten gehören.

Umgekehrt, bedeutet aber nicht, dass wenn sie denselben Erwartungswert haben,
sie zwingend zu derselben Grundgesamtheit gehören. Das zu prüfen erfordert die
in den nachfolgenden Abschnitten behandelten Tests der Varianzen, den $\chi^2$-Test bei
Test der empirischen Varianz einer Stichprobe mit einer bekannten Varianz, den
F-Test bei Test der Varianzen zweier Stichproben.

Der Test auf Erwartungswerte, der \textsl{t}-Test, wird wie folgt formuliert:
\begin{center}
\begin{tabular}{c|cl}
$H_0$ & $\mu_1 = \mu_2$ & beide Stichproben haben den gleichen Erwartungswert\\
\hline
$H_\mathrm{a}$ & $\mu_1 \neq \mu_2$ & beide Stichproben haben unterschiedliche Erwartungswerte und\\
 & & gehören somit unterschiedlichen Grundgesamtheiten an
\end{tabular}
\end{center}

Die Prüfgröße ist die Differenz der empirisch berechneten Mittelwerte normiert auf die
Standardabweichungen der Mittelwerte. Mit Gl.~(\ref{empirischeStd}) wird die
empirische Standardabweichung einer Stichprobe berechnet. Das in Tabelle 1 aufgeführte
Beispiel zeigt, dass die Mittelwerte deutlich weniger streuen, als die Werte innerhalb
einer einzelnen Stichprobe. Die Varianz des Mittelwertes einer Stichprobe wird gemäß
Gl.~(\ref{empirischeVarianzMittelwert})
darüber abgeschätzt, dass sie durch den Stichprobenumfang geteilt wird:
\begin{equation}
\bar s_i^2 \; = \; \frac{1}{J_i (J_i - 1)} \sum_{j=1}^{J_i} \, (X_{i,j} - y_i)^2
\end{equation}
Die Prüfgröße des \textsl{t}-Tests für den Vergleich der beiden Stichproben $i = 1,2$ ist wie folgt definiert
\begin{equation}
T \; = \; \frac{y_1 \, - \, y_2}{\sqrt{\bar s_1^2 + \bar s_2^2}}
\label{tTest}
\end{equation}
mit
\begin{equation}
y_i \; = \; \frac{1}{J_i}\sum_{j=1}^{J_i} \, X_{i,j} \qquad \mathrm{mit} \qquad
i = 1, 2
\end{equation}
Die Nullhypothese wird mit einem Signifikanzniveau von $\alpha = 0.05$ abgelehnt, wenn
der Betrag der Prüfgröße größer als das entsprechende Quantil der \textsl{t}-Verteilung ist
\begin{equation}
|T| \; > \; t_{1-\frac{1}{2} \alpha, \, \nu} \qquad \Rightarrow \qquad H_0 \; \; \mathrm{ablehnen}
\end{equation}


Für den $t$-Test mit zwei Stichproben wird unterschieden nach \textsl{gepoolten} und \textsl{nicht gepoolten}
Stichproben. Unter \textsl{gepoolten} Stichproben (\textsl{Samples}) versteht man diejenigen, deren Varianzen
im wesentlichen als gleich zu betrachten sind.
Die gemeinsame Anzahl der Freiheitsgrade für \textsl{gepoolte} Samples, die für die Wahl des $t$-Quantils gebraucht wird, ist
\begin{equation}
\nu \; = \; J_1 + J_2 - 2
\label{dofpooled}
\end{equation}
und für NICHT \textsl{gepoolte}
\begin{equation}
\nu \; = \;
 \frac{ \left(\frac{s_1^2}{J_1} +\frac{s_2^2}{J_2}\right)^2}{
  \frac{\left(\frac{s_1^2}{J_1}\right)^2}{J_1 - 1} + \frac{\left(\frac{s_2^2}{J_2}\right)^2}{J_2 - 1} } .
\label{dofnonpooled}
\end{equation}
Die Herleitung zur Berechnung der Anzahl der Freiheitsgrade für nicht gepoolte Stichproben
wird in Kapitel \ref{unsicherheitsfortpfLin} dran kommen. Gl.~(\ref{dofnonpooled}) wird auch
Satterthwaite'sche Gleichung genannt.

Bei $\nu = J_1 + J_2 - 2$ Freiheitsgraden, bei dem hier betrachteten Beispiel ist dies
$\nu = 98$ und $t_{1-\frac{1}{2} \alpha, \, \nu} = t_{0.975, 98} = 1.985$.
Der Vergleich der ersten mit der zweiten Stichprobe aus Tabelle 1 liefert
$$
|T| \; = \; \frac{|40.79 \; - \; 43.08|}{\sqrt{\frac{1}{50} (4.83)^2 \; + \; \frac{1}{50} (5.52)^2}}
\; = \; 2.21  \; > \; 1.99
$$
dass die Mittelwerte der beiden Stichproben signifikant, auf einem Signifikanzniveau von
$\alpha = 0.05$, von einander abweichen und die Nullhypothese verworfen wird.

Betrachtet man jedoch einen größeren Vertrauensbereich, also anstelle von $95 \%$ einen
Bereich von $98 \%$, nimmt man also mehr aus dem Bereich der \textsl{Tails} hinzu, so
wird die Nullhypothese nicht verworfen. Auf einem Signifikanzniveau von nur
$\alpha = 0.02$ hat das Vertrauensintervall die Grenzen
$-t_{1-\frac{1}{2} \alpha, \, \nu} = -t_{0.99, 98} = -2.365$ und
$t_{1-\frac{1}{2} \alpha, \, \nu} = t_{0.99, 98} = 2.365$. Die
Nullhypothese wird für diese Wahl des Signifikanzniveaus nicht verworfen,
weil die Prüfgröße innerhalb des breiteren Intervalls mit
$|T| \; = \; 2.21 \; < \; 2.37$ liegt.

\begin{verbatim}
http://www.itl.nist.gov/div898/handbook/eda/section3/eda353.htm
\end{verbatim}

\section{$\chi^2$-Verteilung und Test einer Varianz}


Eine Verteilungsdichtefunktion, die die Verteilung der Quadrate einer
 normierten Zufallsgröße $Z$,
also die Verteilung der Varianzen einer Zufallsgröße $X$, beschreibt, ist die
$\chi^2$-Verteilung, gesprochen \textsl{Chi-Quadrat}-Verteilung.


Bisher haben wir uns damit befasst, wie eine Zufallsgröße $X$ streut.
Jetzt betrachten wir, wie die Streuung ihrerseits streut.
Wir betrachten eine Zufallsgröße, die zu einer normalverteilten Grundgesamtheit gehört.
\begin{quote}
$X$ sei eine eine normalverteilte Zufallsgröße mit Erwartungswert $\mu$ und Varianz $\sigma^2$
\end{quote}
d.h.
\begin{equation}
X \; \sim \; \mathcal{N}(\mu, \sigma).
\end{equation}
Werden einzelne kleinere Stichproben $X_i$ aus der Grundgesamtheit von $X$ entnommen, die
jeweils einen Stichprobenumfang $J_i$ haben und werden die jeweiligen Mittelwerte und
empirischen Varianzen berechnet
\begin{equation}
y_i \; = \; \frac{1}{J_i} \, \sum_{j=1}^{J_i} \, X_{i,j} \qquad
 s_i^2 \; = \; \frac{1}{J_i-1} \, \sum_{j=1}^{J_i} \, (X_{i,j} \, - \, y_i)^2
\end{equation}
so haben wir anhand der zuvor behandelten Beispiele festgestellt,
dass nicht nur die Mittelwerte $y_i$ sondern auch die Varianzen $s_i^2$ streuen.
Die Charakteristik der Verteilungsdichte der Varianzen hängt vom
Stichprobenumfang $J_i$ ab. Der Stichprobenumfang entspricht der Anzahl der Freiheitsgrade.

Gegeben sei eine Stichprobe $Z_1, \dots, Z_J$ unabhängiger Beobachtungen einer
standardnormalverteilten Zufallsgröße $Z  \sim \; \mathcal{N}(0, 1)$:
Dann ist die Summe der Quadrate der Beobachtungen $Q$
\begin{equation}
Q \; = \; \sum_{j=1}^J \, Z_j^2
\end{equation}
gemäß folgender Wahrscheinlichkeitsdichtefunktion verteilt
\begin{equation}
p_\mathrm{\chi^2}(Q, \nu) \; = \; \frac{Q^{\frac{\nu}{2} - 1} \,
 e^{-\frac{Q}{2}}}{2^{\frac{\nu}{2}} \, \Gamma(\frac{\nu}{2})} \qquad Q > 0
\label{Chi2pdf}
\end{equation}
mit $\nu$ für die Anzahl der Freiheitsgrade und $\Gamma$ für die
im Abschnitt zuvor als Gl.~(\ref{GammaHalfInt}) angegebenen Gammafunktion.

Eine Schreibweise für die Aussage
\begin{quote}
$Q$ ist $\chi^2$-verteilt
\end{quote}
ist
\begin{equation}
Q \; \sim \; \chi^2(\nu).
\label{Chi2verteilt}
\end{equation}
Die Definition der $\chi^2$-Verteilungsdichtefunktion braucht nicht auswendig gelernt zu
werden. Der Umgang mit den Quantiltabellen ist aber zu üben.
Wichtig zu wissen ist, dass die $\chi^2$-Verteilungs\-dichte\-funktion für
einen positiven Wertebereich gilt, sie ihr Maximum in der Nähe von $Q = J$ hat, sie um so
schiefer und breiter ist, je kleiner $J$ ist und die Verteilungsdichte der Varianzen
\begin{equation}
s_{\mu,i}^2 \; = \; \frac{1}{J_i} \, \sum_{j=1}^{J_i} \, (X_{i,j} \, - \, \mu)^2
\end{equation}
\begin{equation}
s_{\mu}^2 \; = \; \frac{1}{J} Q \, \sigma^2  \qquad \Leftrightarrow \qquad
Q \; = \; J \, \left( \frac{s_{\mu}}{\sigma} \right)^2
\label{s2Q}
\end{equation}
liefert. Was in der Praxis hinsichtlich des Statistiktests, dem $\chi^2$-Test, gebraucht wird,
ist das Verständnis wie die Tabellen verwendet werden
und für Methoden der bayesischen Statistik erforderlichenfalls auch wie die
Funktionen der jeweils eingesetzten Numerik-/Statistik\-biblio\-theken zu benutzen sind.

\begin{figure}
\begin{center}
\includegraphics[width=75mm]{05_vorlesung/media/understandchi2_df9_final.pdf} \hspace{5mm}
\includegraphics[width=75mm]{05_vorlesung/media/understandchi2_df20_final.pdf}
\caption{\label{ch2beispiele}$\chi^2$-Verteilung der normierten
Zufallsgröße $Q \; = \; J \, \left( \frac{s_{\mu}}{\sigma} \right)^2$ für
$s_{\mu}^2 \; = \; \frac{1}{J} \, \sum_{j=1}^{J} \, (X_{..,j} \, - \, \mu)^2$
für $J = 9$  (\textsl{links}) und $J = 20$ (\textsl{rechts}).}
\end{center}
\end{figure}

Abb.~\ref{ch2beispiele} zeigt zwei Beispiele für die Verteilungsdichte der Varianzen, zum einen
für $J = 9$ und zum anderen für $J = 20$. Mit
%\lstset{language=Matlab}
\begin{lstlisting}[style=Matlab]
function understandchi2()
  Jz = 9;
  nbin = 100;
  n = 10000;
  d = zeros(n,1);
  for k=1:n
    x = randn(Jz,1);
    d(k) = sum(x.^2);
  end
  [haeuf, bin] = hist(d,nbin);
  Deltabin = bin(2)-bin(1);
  figure(1000);
  plot(bin,haeuf/(n*Deltabin),'bd-',bin,chi2pdf(bin,Jz),'r-');
  xlabel('J * (s / \sigma)^2', 'fontsize', 14);
  ylabel('Wahrscheinlichkeitsdichte', 'fontsize', 14);
  title([num2str(n) 'Stichprobenumfang J = \nu = ' num2str(Jz)], 'fontsize', 14);
  grid on;
  set(gca, 'fontsize', 14, 'linewidth', 2);
  print(1000,['understandchi2_df' num2str(Jz) '_solid.svg'],'-dsvg');
end
\end{lstlisting}
wurden die Diagramme erzeugt. Die Verteilung der Grundgesamtheit ist die
Standardnormalverteilung mit $\mu = 0$ und $\sigma = 1$.
Eine Stichprobenentnahme wird mit \texttt{x = randn(Jz,1);} simuliert.

Wie bei der Quantildefinition für die Standardnormalverteilung und für die $t$-Verteilung
ist das Quantil der $\chi^2$-Verteilung die obere Integrationsgrenze
zur Gewinnung des Flächeninhalts der Verteilungsdichte. Dies quantifiziert die Wahrscheinlichkeit,
mit der für die Größe $Q$ Werte beobachtet werden, die kleiner sind als das Quantil.
Hier ist die untere Integrationsgrenze Null und nicht minus Unendlich, weil $Q$ per
Definitionem nur positive Werte haben kann, siehe Abb.~\ref{chi2quantil}.
\begin{figure}
\begin{center}
\includegraphics[width=100mm]{05_vorlesung/media/chiquadrat.pdf}
\caption{\label{chi2quantil}Wahrscheinlichkeitsverteilungsdichte (pdf) und
kumulierte Wahrscheinlichkeitsfunktion (cdf)
der normierten Varianzen, das Quantil wird $\chi^2$ genannt. Hier für
$\nu = 9$ Freiheitgrade und $1 - \alpha = 0.7$, somit $\chi^2_{\nu, 1-\alpha} = 10.66$
dargestellt.}
\end{center}
\end{figure}
\begin{equation}
P(Q) \; = \;
\int \limits_0^{\chi^2_{1-\alpha, \nu}}
p(Q^\prime, \nu) \; \operatorname{d} Q^\prime \; = \;
1 \, - \, \alpha
\label{chiQuadratQuantil}
\end{equation}
Bei dem $\chi^2$-Test wird die empirische Varianz einer Stichprobe
\begin{equation}
s_{\mu,i}^2 \; = \; \frac{1}{J_i} \, \sum_{j=1}^{J_i} \, (X_{i,j} \, - \, \mu)^2
\; \approx \; \frac{1}{J_i-1} \, \sum_{j=1}^{J_i} \, (X_{i,j} \, - \, y_i)^2
\; = \; s_i^2
\end{equation}
mit der bekannten Varianz einer Grundgesamtheit verglichen.
\begin{lstlisting}[style=Matlab]
  for k=1:n
    x = randn(Jz,1);
    y = mean(x);
    d(k) = sum((x-y).^2);
  end
\end{lstlisting}
Dann ist die Anzahl der Freiheitsgrade $\nu = J-1$ um einen vermindert,
weil der Mittelwert $y_i$ von den Werten $X_{i,j}$ abhängt. Die $\chi^2$-Verteilung
ist mit der Anzahl der Freiheitsgrade und nicht mit dem Stichprobenumfang zu verwenden
\begin{equation}
s^2 \; = \; \frac{1}{\nu} Q \, \sigma^2  \qquad \Leftrightarrow \qquad
Q \; = \; \nu \, \left( \frac{s}{\sigma} \right)^2 ,
\label{s2Qempir}
\end{equation}
so dass die $\chi^2$-Verteilungsdichte  für $\nu = J-1$ zu verwenden ist,
in Gnu-Octave beispielsweise mit dem Funktionsaufruf
\texttt{chi2pdf}:

\begin{lstlisting}[style=Matlab]
  [haeuf, bin] = hist(d,nbin);
  Deltabin = bin(2)-bin(1);
  plot(bin,haeuf/(n*Deltabin),'bd-',bin,chi2pdf(bin,Jz-1),'r-');
  xlabel('(J-1) * (s / \sigma)^2', 'fontsize', 14);
\end{lstlisting}

Beim $\chi^2$-Test wird geprüft, ob die Varianz einer Stichprobe gleich einer
spezifizierten Varianz $\sigma_0^2$ ist. Wir formulieren die Hypothesen
\begin{center}
\begin{tabular}{c|cl}
$H_0$ & $\sigma^2 = \sigma_0^2$ & die Stichprobe gehört zu einer Grundgesamtheit mit der Varianz $\sigma_0^2$\\
\hline
$H_\mathrm{a}$ & $\sigma^2 \neq \sigma_0^2$ & die Stichprobe gehört nicht zu einer Grundgesamtheit mit der Varianz $\sigma_0^2$
\end{tabular}
\end{center}
Die Testgröße ist
\begin{equation}
T \; = \; \nu \, \left( \frac{s}{\sigma_0} \right)^2 .
\label{chi2testgroesse}
\end{equation}
Die Nullhypothese wird mit einem Signifikanzniveau von $\alpha$
verworfen, falls die Testgröße außerhalb des Intervalls liegt,
das durch die in Gl.~(\ref{chiQuadratQuantil}) definierten Quantile begrenzt wird, d.h.
\begin{equation}
T \; < \; \chi^2_{\alpha/2, \nu}  \qquad \mathrm{oder} \qquad
T \; > \; \chi^2_{1-\alpha/2, \nu} .
\end{equation}
Es ist zu beachten, dass beim zweiseitigen Test für jede Seite die
Hälfte des Signifikanzniveaus verwendet wird, also $\alpha/2$.

\begin{verbatim}
http://www.itl.nist.gov/div898/handbook/eda/section3/eda358.htm
\end{verbatim}


\section{Fisherverteilung und Vergleich zweier empirischer Varianzen}

Der Fall, bei dem zu überprüfen ist, ob die Varianzen zweier Stichproben zur gleichen
Grundgesamtheit gehören müssten, bzw.\ ob diese im wesentlichen übereinstimmen, wird
als Hypothesentest wie folgt formuliert:
\begin{center}
\begin{tabular}{c|cl}
$H_0$ & $\sigma_1^2 = \sigma_2^2$ & die beiden Stichproben haben die gleichen Varianzen\\
\hline
$H_\mathrm{a}$ & $\sigma_1^2 \neq \sigma_2^2$ & die Stichproben haben unterschiedliche Varianzen
und \\
 &  & gehören somit zu unterschiedlichen Grundgesamtheiten
\end{tabular}
\end{center}
Die Testgröße ist
\begin{equation}
T \; = \; \left( \frac{s_1}{s_2} \right)^2.
\label{Ftestgroesse}
\end{equation}
Sie wird verglichen mit den beiden Quantilen, dem linksseitigen und dem rechtsseitigen, der Fisherverteilung.
Der Test wird in der Literatur Fisher-Test oder kurz F-Test genannt.

Die Fisherverteilung ist die Marginalverteilung des Produkts der Wahrscheinlichkeitsdichtefunktionen zweier
$\chi^2$-verteilter Zufallsgrößen $Q_1$ und $Q_2$
\begin{equation}
p_\mathrm{F}(R, \nu_1, \nu_2) \; = \; \int\limits_0^{\infty}
p_\mathrm{\chi^2}(R, \nu_1) \, p_\mathrm{\chi^2}(Q_2, \nu_2) \; \frac{\nu_1}{\nu_2} Q_2 \;
\mathrm{d} Q_2
\label{FisherverteilungAnsatz}
\end{equation}
mit
\begin{equation}
R  \; = \; \frac{\frac{1}{\nu_1} Q_1}{\frac{1}{\nu_2} Q_2} .
\label{TrafoQ1Q2}
\end{equation}
Dabei ist der Faktor $\frac{\nu_1}{\nu_2} Q_2$
die Funktionaldeterminante aus der Transformation $(Q_1, Q_2) \rightarrow (R, Q_2)$,
um aus der Wahrscheinlichkeitsdichtefunktion der beiden $\chi^2$-verteilten Zufallszahlen
$$
p_\mathrm{\chi^2}(Q_1, \nu_1) \, p_\mathrm{\chi^2}(Q_2, \nu_2)
$$
die Fisherverteilung zu berechnen. Nach Einsetzen der Gleichung für
die $\chi^2$-Verteilung, Gl.~(\ref{Chi2pdf}), und Ausführen der Integration
in Gl.~(\ref{FisherverteilungAnsatz}), sieht die Fisherverteilung wie folgt aus
\begin{equation}
p_\mathrm{F}(R, \nu_1, \nu_2) \; = \;
\nu_1^{\frac{\nu_1}{2}} \nu_2^{\frac{\nu_2}{2}} \,
\frac{\Gamma\left(\frac{\nu_1+\nu_1}{2}\right)}
{\Gamma\left(\frac{\nu_1}{2}\right)\Gamma\left(\frac{\nu_2}{2}\right)}
\frac{R^{\frac{\nu_1}{2}-1}}{\left(\nu_1 R + \nu_2 \right)^\frac{\nu_1+\nu_1}{2}} .
\label{Fisherpdf}
\end{equation}
Abb.~\ref{Fverteilbeispiel} zeigt beispielhaft zwei Fisherverteilungen, die mit
blau gestrichelter Kurve dargestellte für $\nu_1 = 8$ und $\nu_2 = 19$ und die mit
schwarz durchgezogener Kurve für $\nu_1 = \nu_2 = 8$ Freiheitsgrade.

\begin{figure}
\begin{center}
\includegraphics[width=75mm]{05_vorlesung/media/BeispielFverteilung.pdf}
\caption{\label{Fverteilbeispiel}Fisherverteilung des Quotienten $R$
zweier $\chi^2$-verteilter Zufallsgrößen $Q_1$ und $Q_2$, also
$R \; = \; \frac{\frac{1}{\nu_1} Q_1}{\frac{1}{\nu_2} Q_2} \;
= \; \frac{s_1^2}{s_2^2}$ mit jeweils der Anzahl der Freiheitsgrade
$\nu_1$ und $\nu_2$. Die beiden Kurven zeigen zwei Beispiele, die
blau gestrichelte für $\nu_1 = 8$ und $\nu_2 = 19$ und die schwarze durchgezogene
für $\nu_1 = \nu_2 = 8$ Freiheitsgrade.}
\end{center}
\end{figure}



Die Nullhypothese wird mit einem Signifikanzniveau von $\alpha$
verworfen, falls die Testgröße außerhalb des Intervalls liegt,
das durch die Quantile der Fisherverteilung begrenzt wird, d.h.
\begin{equation}
T \; < \; F_{\alpha/2, \nu_2, \nu_1}  \qquad \mathrm{oder} \qquad
T \; > \; F_{1-\alpha/2, \nu_2, \nu_1} .
\end{equation}
Dabei ist $\nu_1$ die Anzahl der Freiheitsgrade zu $s_1$, die
im Zähler stehende Standardabweichung
aber in den Tabellen Nennerfreiheitsgrad genannt wird, und $\nu_2$
die Anzahl der Freiheitsgrade zu $s_2$, die im Nenner stehende
Standardabweichung deren Freiheitsgrad
aber in den Tabellen Zählerfreiheitsgrad genannt wird. Das kommt
daher, dass in Gl.~(\ref{TrafoQ1Q2}) die Freiheitsgrade $\nu_1$ zur
Zufallsgröße $Q_1$ im Nenner stehen.

Beispiel:
Der \textsl{General Social Survey (GSS)} hatte in den USA im Jahr 2000 anhand einer
zufallsgenerierten Teilstichprobe mit $J = 652$ Erwachsenen Menschen untersucht,
wie sich die Nutzungsdauer $t$ in Stunden/Woche der Internetnutzung nach Geschlecht unterscheidet.

\begin{center}
\begin{tabular}{l|ccc}
Geschlecht & $J_i$ & $t_i$ & $s_i$\\
\hline\hline
männlich & $305$ & $5.48$ & $7.80$\\
\hline
weiblich & $347$ & $3.84$ & $5.86$
\end{tabular}
\end{center}

mit $J_i$ für die Stichprobenumfänge und $s_i$ für die Standardabweichung mit $i = 1,2$.
Die Standardabweichung zur Nutzungsdauer $t$ beträgt bei den Männern $t_1 = 5.48$  Stunden/Woche
mit einer Streuung von
$s_1 = 7.80$ Stunden/Woche und bei den Frauen
$t_1 = 3.84$  Stunden/Woche mit einer Streuung von $s_2 = 5.86$ Stunden/Woche.
Der F-Test überprüft, ob der Unterschied der Varianzen $s_1^2$ und $s_2^2$ signifikant ist.
Im vorliegenden Beispiel beträgt der Nennerfreiheitsgrad $\nu_1 = J_1 - 1 = 305-1 = 304$ und der
Zählerfreiheitsgrad $\nu_2 = J_2 - 1 = 347-1 = 346$.

\begin{lstlisting}[style=Python]
import numpy as np
import scipy.stats
alpha = 0.05
s1 = 7.80
s2 = 5.86
J1 = 305
J2 = 347
nu1 = J1-1
nu2 = J2-1
Tpruef = (s1/s2)**2
nuZaehl = nu2
nuNenn = nu1
Flinks = scipy.stats.f.ppf(alpha/2, nuZaehl, nuNenn)
Frechts = scipy.stats.f.ppf(1-alpha/2, nuZaehl, nuNenn)
print(Flinks, Tpruef, Frechts)
\end{lstlisting}
$T = 1.772$ und $F_{\alpha/2, \nu_2, \nu_1} = 0.805$ und
$F_{1-\alpha/2, \nu_2, \nu_1} = 1.245$. Mit $T = 1.772 \; > \; F_{1-\alpha/2, \nu_2, \nu_1} = 1.245$
liegt die Prüfgröße signifikant höher als das rechte Quantil.
Es kann also davon ausgegangen werden, dass sich die Varianzen der beiden Geschlechter signifikant voneinander unterscheiden.

\section{Anwendung von Hypothesentests}

Anwendung finden die Hypothesentests im Bereich der Qualitätssicherung in der Produktion (siehe beispielsweise
zur Prüfung des Stout in der Guinnessbrauerei schon vor 100 Jahren), oder im Bereich der Naturwissenschaften
(Biologie, Chemie, Pharmazie, Medizien) zum Nachweis von Substanzen. Es geht darum, dass zu bewerten ist,
mit welcher Wahrscheinlichkeit Beobachtungen zu welcher Verteilung gehören.
Einer Entscheidung liegen Regeln zugrunde. Mit dem $t$-Test haben wir die Regeln kennengelernt, über
die wir entscheiden, ob zwei Datensätze den gleichen Erwartungswert haben.

Als nächstes wollen wir untersuchen, welche Fehler mit welcher Wahrscheinlichkeit dabei entstehen können.
Anschließend wollen wir uns ansehen, wie eine Stichprobe mit einer Toleranzvorgabe verglichen wird.
Die Toleranzvorgabe liefert ein Intervall, dessen Lage mit der Lage der Wahrscheinlichkeitsverteilung
der zu untersuchenden Stichprobe verglichen wird.

\subsection{Entscheidung bei Vergleich zweier Stichproben}
\begin{figure}
\begin{center}
\includegraphics[width=100mm]{05_vorlesung/media/entscheidungsfehlertypen.pdf}
\caption{\label{entscheidungsfehler} Die Ereignisse (beobachtete Werte) aus der linken Wahrscheinlichkeitsdichteverteilung,
 die in deren \textsl{Tails} liegen, werden mit einem Signifikanzniveau von $\alpha$ verworfen.
 Die Ereignisse (beobachtete Werte) aus der rechten Wahrscheinlichkeitsdichteverteilung, die in dessen linkem
 \textsl{Tail} liegen, werden mit Wahrscheinlichkeit $\beta$ als der linken Wahrscheinlichkeit zugehörig
 erachtet.}
\end{center}
\end{figure}
Den Hypothesentests, die wir uns angesehen haben, ist gemeinsam, dass
\begin{enumerate}
\item eine Nullhypothese $H_0$ (evtl.\ auch eine Alternativhypothese $H_\mathrm{a}$)
aufgestellt wurde,
\item ein Signifikanzniveau $\alpha$ spezifiziert wurde,
\item eine Stichprobe mit einer Anzahl Freiheitsgrade $\nu$
genommen wurde und
\item anhand der vorher aufgestellten Entscheidungsregeln (Vergleich einer
Prüfgröße mit dem Quantil einer Wahrscheinlichkeitsdichteverteilung)
die Hypothese verworfen oder angenommen wurde.
\end{enumerate}
In der Entscheidungsregel werden durch Vorgabe eines Signifikanzniveaus Verwerfungsbereich
und Annahmebereich festgelegt. Das Signifikanzniveau ist dabei Komplementärwahrscheinlichkeit
(Gegenwahrscheinlichkeit) zum Vertrauensniveau, das ist die Wahrscheinlichkeit dafür
wie sicher die Hypothese ist.

Die Wahrscheinlichkeit, mit einem Hypothesentest eine falsche
Entscheidung zu treffen wird in zwei Klassen von Fehlern aufgeteilt, den
$\alpha$-Fehler oder den $\beta$-Fehler

\begin{center}
\begin{tabular}{M{3cm}|M{5.5cm}|M{5.5cm} N}
                 &     $H_0$ annehmen    &     $H_0$ ablehnen   \\[3pt]
\hline
$H_0$ ist wahr   & richtige Entscheidung, Wahrscheinlichkeit: $1-\alpha$ & Fehler 1.\ Art, Wahrscheinlichkeit $\alpha$ \\[3pt]
\hline
$H_0$ ist falsch & Fehler 2.\ Art, Wahrscheinlichkeit: $\beta$  & richtige Entscheidung, Wahrscheinlichkeit: $1-\beta$
\end{tabular}
\end{center}

Abb.~\ref{entscheidungsfehler} soll illustrieren, dass Beobachtungswerte, die zu einer
Grundgesamtheit mit einer Wahrscheinlichkeitsdichteverteilung mit Erwartungswert
$\mu_\mathrm{a} = 3.5$ gehören, per Hypothesentest einer Grundgesamtheit mit $\mu_0 = 0$
zugeordnet werden. Die Wahrscheinlichkeit für das Auftreten solcher Beobachtungen ist
dann $\beta$. Das Diagramm wurde mit folgendem Octaveskript erzeugt

\begin{lstlisting}[style=Matlab]
function plot_entscheidungsfehler()
  dz = 0.02;
  lim = 7;
  z = [-lim:dz:lim];
  p1 = normpdf(z,0,1);
  t = 1.96;
  zleft = [-lim:dz:-t];
  pleft = normpdf(zleft,0,1);
  zright = [t:dz:lim];
  pright = normpdf(zright,0,1);
  mu2 = 3.5;
  p2 = normpdf(z+mu2,mu2,1);
  zbeta = [mu2-lim:dz:t];
  pbeta = normpdf(zbeta,mu2,1);
  figure(200); hold on;
  area( zleft, pleft, 'Facecolor', [0.5 0.5 1]);
  area( zright, pright, 'Facecolor', [0.5 0.5 1]);
  area( zbeta, pbeta, 'Facecolor', [1 0.5 0.5]);
  plot( z, p1, 'k-', 'linewidth', 2);
  plot( z+mu2, p2, 'k--', 'linewidth', 2);
  xlabel('X', 'fontsize', 14);
  ylabel('p', 'fontsize', 14);
  axis([-4 7 0 0.5]);
  set(gca, 'fontsize', 14);
  hold off;
  print(200, 'entscheidungsfehler.svg', '-dsvg');
end
\end{lstlisting}


\subsection{Konformitätsprüfung}
Nachdem wir gesehen haben wie wir zwei Hypothesen über die Lage zweier Wahrscheinlich\-keits\-dichte\-ver\-teilungen
verglichen haben, befassen wir uns jetzt mit dem Vergleich von Grenzen, den Toleranzgrenzen, mit einer
Wahrscheinlichkeitsdichteverteilung. Für die Fertigung werden in den Konstruktionszeichnungen zu den Merkmalen
von Bauteilen die Toleranzgrenzen eingetragen, innerhalb derer die Merkmale des Werkstücks liegen müssen. Wie nun
bewertet man nach Fertigstellung des Werkstücks, ob ein Merkmal innerhalb einer Toleranzvorgabe liegt? Ein Merkmal
wird durch einen Messvorgang geprüft. Der Messvorgang liefert ein Ergebnis: Einen Zahlenwert für das Merkmal und
ein Überdeckungsintervall zusammen mit einem Vertrauensniveau. Anstelle von Überdeckungsintervall und Vertrauensniveau
kann auch direkt eine Wahrscheinlichkeitsdichteverteilung angegeben werden, ist aber sehr unüblich in der industriellen
Praxis. Für die Bewertung, ob das gefertigte Werkstück mit der Vorgabe der Konstruktion übereinstimmt, ob es konform ist,
werden Messergebnis und Toleranzvorgabe verglichen.

Als Ergänzung (\textsl{Supplement}) zum \glqq Guide of Uncertainty\grqq ~gibt es das Dokument
\textsl{Evaluation of measurement data - The role of measurement uncertainty in conformity assessment},
das darlegt, wie Konformitätsbewertungen vorzunehem sind.
\begin{verbatim}
	https://www.bipm.org/utils/common/documents/jcgm/JCGM_106_2012_E.pdf
\end{verbatim}
Dort heißt es in Paragraph 7.1.1
\begin{quote}
	An item conforms to a specified requirement,
	if the true value of its associated property $Y$ lies in the tolerance
	interval. Knowledge of $Y$ is conveyed by a probability density function $p(y|{X_1,\dots,X_J})$
	so that a statement of conformity is always an inference,
	with some probability of being true. Denoting the set of permissible (conforming) values
	$Y$ by $C$, the conformance probability, denoted by $p_\mathrm{c}$, is given by
	\begin{equation}
		p_\mathrm{c} = P(Y \in C | {X_1,\dots,X_J}) = \int\limits_C p(y|{X_1,\dots,X_J}) \mathrm{d}y.
	\end{equation}
\end{quote}
Im Originaldokument werden Sie eine etwas andere Schreibweise für die Bezeichner in der Formel vorfinden.
Hier ist es so aufgeschrieben, wie es zu der Notation innerhalb dieses Vorlesungsskriptes, insbesondere
zu Kapitel \ref{KonzepteinverseProbleme}, passt.
Für die Toleranz wird in der Richtlinie zur Konformitätsbewertung zur Wahrung der Allgemeingültigkeit
eine Menge $C$ angegeben. Falls die Größe $Y$ eine skalarwertige Größe wie bei dem Beispiel, anhand dessen
wir in Kapitel \ref{KonzepteinverseProbleme} das Prinzip der bayesischen Statistik illustriert haben,
so steht $C$ für ein Intervall $C = [T_\mathrm{L}, T_\mathrm{U}]$ im Falle der zweiseitigen
Grenzen, $C = [-\infty, T_\mathrm{U}]$ für den Fall, dass es eine Obergrenze gibt,
$C = [T_\mathrm{L}, \infty]$, dass es eine Untergrenze gibt. Dabei sollen der Index L für \glqq lower
limit\grqq ~und der Index U für \glqq upper limit\grqq ~stehen. Der Bezeichner $T$ ist in der
Normung das üblicherweise für Toleranzgrenzen verwendete Symbol und bedeutet an dieser
Stelle nicht Testgröße! Er unterscheidet sich von den Testgrößen durch seine Indizes.

Die Wahrscheinlichkeit $p_\mathrm{c}$, dass die Beobachtungen ${X_1,\dots,X_J}$ innerhalb des Toleranzintervalls
$C = [T_\mathrm{L}, T_\mathrm{U}]$ liegen, ist
\begin{equation}
	p_\mathrm{c} =  \int\limits_{T_\mathrm{L}}^{T_\mathrm{U}} p(y|{X_1,\dots,X_J}) \mathrm{d}y.
\end{equation}
Für den Fall, dass von einer normalverteilten Stichprobe ausgegangen werden kann und dass eine Einzelgröße
vorliegt, wird auch der Stichprobe ${X_1,\dots,X_J}$ der Schätzer $\bar y$ aus einfacher Mittelwertbildung
gewonnen ($\bar y = \frac{1}{J} \sum\limits_{j=1}^J X_j$) und die Varianz als empirische Varianz $s$ aus
$s^2 = \frac{1}{J-1}\sum\limits_{j=1}^J (X_j - \bar y)^2$ berechnet
und für die Wahrscheinlichkeitsdichte $p$ die Gaußverteilung verwendet
\begin{equation}
	p_\mathrm{c} = \frac{1}{\sqrt{2 \pi} s} \int\limits_{T_\mathrm{L}}^{T_\mathrm{U}}
	e^{-\frac{1}{2}\left(\frac{y - \bar y}{s}\right)^2} \mathrm{d}y .
		\label{eq:Konformitaetswahrscheinlichkeit}
\end{equation}
Die Größe $\frac{y - \bar y}{s}$ ist eine normierte Zufallsgröße. Die Integrationsgrenzen können gleichfalls
normiert werden: $z = \frac{y - \bar y}{s}$, $z_\mathrm{L} = \frac{T_\mathrm{L} - \bar y}{s}$ und
$z_\mathrm{U} = \frac{T_\mathrm{U} - \bar y}{s}$, so dass die Tabellenwerke für die kumulative
standardnormalverteilte Gaußfunktion oder die Bibliotheksfunktion
für die gauß'sche Fehlerfunktion (\textsl{error function}) wie folgt verwendet werden kann:
\begin{equation}
	p_\mathrm{c} = \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{z_\mathrm{U}}
	e^{-\frac{1}{2} z^2} \mathrm{d}z -
	\frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{z_\mathrm{L}} e^{-\frac{1}{2} z^2} \mathrm{d}z
\end{equation}
d.h.
\begin{equation}
	p_\mathrm{c} = \Phi(z_\mathrm{U}) - \Phi(z_\mathrm{L})
\end{equation}
mit $\Phi(z) = \frac{1}{2} \left(1 + \mathrm{erf}\left(\frac{z}{\sqrt{2}}\right) \right)$. Die Fehlerfunktion ist definiert als
\begin{equation}
	\mathrm{erf}(z)= \frac{2}{\sqrt{\pi}}\int\limits_{0}^{z} \exp (-t^2)\mathrm{d}t
\end{equation}
Die Standardnormalverteilung $\mathcal{N}(\mu=0,\sigma^2=1)$ haben wir mit $\Phi(z)$ bezeichnet. Sie ist wie folgt gegeben:
\begin{equation}
	\Phi(z)= \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{z} \exp (-t^2)\mathrm{d}t
\end{equation}
Mit Hilfe der Toleranzintervalle unterscheidet man zwischen Werten, die mit
Vorgaben beispielsweise der Konstruktion konform sind, und nicht konformen Werten.
Im Toleranzintervall liegen die konformen Werte. Außerhalb des Toleranzintervalls liegen die nicht konformen Werte. Man kann prinzipiell drei Fälle unterscheiden: (a) einseitiges Toleranzintervall mit einer unteren (engl.\ \textsl{lower}) Toleranzgrenze $T_\mathrm{L}$; (b) einseitiges Toleranzintervall mit einer oberen (engl. \textsl{upper}) Toleranzgrenze $T_\mathrm{U}$; (c) zweiseitiges Toleranzintervall mit einer unteren und einer oberen Toleranzgrenze. Die Differenz $T_\mathrm{U} - T_\mathrm{L}$ bezeichnet man als Toleranz.
In Abb.~\ref{fig:Toleranzintervalle} sind diese drei Fälle dargestellt.

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[width=130mm]{05_vorlesung/media/Toleranzintervalle.png}
		\caption{\label{fig:Toleranzintervalle}	Toleranzintervalle} (a) einseitiges Toleranzband mit unterer Toleranzgrenze, (b) einseitiges Toleranzintervall mit oberer Toleranzgrenze (c) zweiseitiges Toleranzintervall
	\end{center}
\end{figure}

\textbf{Beispiel: Geschwindigkeitsmessung} \\
\textsl{Aufgabe:} Berechnen Sie die Konformitätswahrscheinlichkeit $p_\mathrm{c}$ bei einer
Geschwindigkeitsmessung. Die zulässige Geschwindigkeit sei 50~km/h. Der gemessene Wert des Autos liegt bei 52~km/h. Die Geschwindigkeitsmessung sei normalverteilt mit einer Standardunsicherheit $u_y= 1~\mathrm{km/h}$. Berechnen Sie die Konformitätswahrscheinlichkeit
$p_\mathrm{c}$, d.h. die Wahrscheinlichkeit, dass das Auto innerhalb der zugelassenen Geschwindigkeit gefahren ist. \\
\textsl{Lösung:} \\
Geschwindigkeitsbegrenzung: $T_\mathrm{U} = 50~\textrm{km/h}$\\
Gemessener Wert: $\bar y = 52~\textrm{km/h}$ \\
Standardmessunsicherheit: $u_y = 1~\textrm{km/h} $ \\
Die Konformitätswahrscheinlichkeit ergibt sich dann zu:
\[
p_\mathrm{c} = \Phi \left( \frac{T_\mathrm{U}-\bar y}{u_y}\right) = \Phi(-2) = 0.023
\]
Der gemessene Wert von 52~km/h liegt mit einer Wahrscheinlichkeit von 2,3~\% innerhalb des To\-le\-ranz\-inter\-valls, siehe Abb.~\ref{fig:LoesungGeschwindigekeitsmessung}
\begin{figure}[!htp]
	\begin{center}
		\includegraphics[width=60mm]{05_vorlesung/media/Bsp_Geschwindigkeitsmessung.png}
		\caption{\label{fig:LoesungGeschwindigekeitsmessung} Beispiel: Konformitätswahrscheinlichkeit bei einer Geschwindigkeitsmessung}
	\end{center}
\end{figure}

\subsection{Konformitätswahrscheinlichkeit und Risiko}
Die wahrscheinlichkeitsbasierte Betrachtung der Konformität ermöglicht die \textsl{Quantifizierung des Risikos einer Fehlbewertung}. Werden beispielsweise bei der Produktion von Widerständen die Widerstandswerte gemessen, so kann ein Toleranzintervall $[T_\mathrm{L}, T_\mathrm{U}]$ definiert werden, in denen die Widerstände noch ok sind und somit auf den Markt gebracht werden. Für den Fall eines einseitigen Toleranzintervalls ist dies in Abb.~\ref{fig:Produktion_Widerstaende} schematisch
dargestellt. Das Risiko einer Fehlbewertung ist dann gegeben durch $1- p_\mathrm{c}$.\\

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[width=80mm]{05_vorlesung/media/Fehlbewertung.png}
		\caption{\label{fig:Produktion_Widerstaende} Beispiel einer Messung bei einseitigem Toleranzintervall. Der schraffierte Bereich zeigt die Konformitätswahrscheinlichkeit $p_\mathrm{c}$. Das Risiko einer Fehlbewertung ist gegeben durch $1- p_\mathrm{c}$}
	\end{center}
\end{figure}

\begin{quote}
Durch Einführen von Akzeptanzgrenzen und Sicherheitsabständen lässt sich das Risiko einer Fehlbewertung steuern.
\end{quote}

Eine Fehlbewertung kann sowohl eine fälschliche Annahme als auch eine fälschliche Ablehnung sein. Das im folgenden dargestellte Konzept stammt aus dem Dokument \cite{JCGM106}. Es werden -- neben den \textbf{Toleranzgrenzen} $T_\mathrm{L}$ und $T_\mathrm{U}$ -- unterere und oberere \textbf{Akzeptanzgrenzen} $A_\mathrm{L}$,\;$A_\mathrm{U}$ eingeführt.
In folgender Abb.~\ref{fig:Toleranz_Akzeptanzintervall} ist dies schematisch verdeutlicht.
\begin{figure}[!htp]
	\begin{center}
		\includegraphics[width=90mm]{05_vorlesung/media/Toleranz_Akzeptanzintervall.png}
		\caption{\label{fig:Toleranz_Akzeptanzintervall} Toleranz- und Akzeptanzintervall. Die Differenz zwischen Toleranzgrenze und Akzeptanzgrenze wird als Sicherheitsabstand / Sicherheitsband bezeichnet.}
	\end{center}
\end{figure}
Die Differenz zwischen Toleranzgrenze und Akzeptanzgrenze definiert einen Längenparameter und wird Sicherheitsabstand $w$ genannt:
\begin{equation}
	w= 	T_\mathrm{U} - A_\mathrm{U}
\end{equation}
In vielen Anwendungen wird dieser Längenparameter als ein Vielfaches der erweiterten Messunsicherheit $U$ angenommen \cite{JCGM106}.
Mit der Wahl der Lage des Akzeptanzintervalls wird das Risiko einer Fehlentscheidung der jeweiligen Situation angemessen ausbalanciert. Es gibt 4 mögliche Resultate einer Konformitätsbewertung. Da wir den tatsächlichen (\glqq wahren\grqq) Wert nicht kennen,
können wir nur auf Grund der Messung entscheiden. Da die Messung jedoch mit einer Messunsicherheit verbunden ist, erhält man bei der Messung eben nicht den wahren Wert.
Es kann dadurch bspw. der Fall eintreten, dass ein Produkt eigentlich innerhalb der zulässigen Toleranzen liegt, jedoch durch die Messung ausgesondert wird. In Abb.~\ref{fig:Resultate_Konformtitaetsbewertung} sind die 4 Resultate einer Konformitätsbewertung dargestellt.

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[width=110mm]{05_vorlesung/media/Resultate_der_KFB.png}
		\caption{\label{fig:Resultate_Konformtitaetsbewertung} Resultate der Konformitätsbewertung am Beispiel einseitiger Akzeptanz- und Toleranzgrenzen. (a) Korrekte Annahme, (b) Falsche Annahme -> Konsumentenrisiko, (c) Korrekte Rückweisung, (d) Falsche Rückweisung -> Produzentenrisiko}
	\end{center}
\end{figure}

\begin{itemize}
	\item[(a)] \textbf{Korrekte Annahme / Akzeptanz}: In diesem Fall liegen gemessener und tatsächlicher Wert innerhalb des Toleranzbandes. Dies ist eine richtige Entscheidung.
	\item[(b)] 	\textbf{Falsche Annahme / Akzeptanz}: Der gemessene Wert liegt innerhalb des Akzeptanzbandes, der tatsächliche Wert jedoch außerhalb des Toleranzbandes. Hier wurde bspw. das Produkt angenommen, obwohl es außerhalb der Spezifikationen ist. Dieses Produkt würde auf den Markt gebracht werden, obwohl es nicht innerhalb der Toleranzen ist. Der Konsument hat ein gewisses Risiko, dass das Produkt, das er erhält, nicht konform ist, obwohl es bei einem Inspektionsprozess als konform bewertet wurde. Man spricht hier deshalb auch vom \textbf{Konsumentenrisiko}.
	\item[(c)] 	\textbf{Korrekte Rückweisung}: In diesem Fall liegt der gemessene Wert noch im Toleranzintervall. Das Produkt wird trotzdem verworfen, da es außerhalb des Akzeptanzintervalls liegt. Da der tatsächliche Wert hier außerhalb des Toleranzintervalls liegt, handelt es sich um eine richtige Entscheidung.
	\item[(d)] 	\textbf{Falsche Rückweisung}: Hier liegen gemessener und tatsächlicher Wert außerhalb des Akzeptanzbandes, jedoch noch innerhalb des Toleranzbandes. Das Produkt wird hier, da es außerhalb des Akzeptanzbandes liegt, fälschlicherweise als Ausschuss deklariert. Der Produzent hat stets ein gewisses Risiko, dass ein Produkt durch den Inspektionsprozess fälschlicherweise als Ausschuss deklariert wird. Deshalb spricht man hier vom \textbf{Produzentenrisiko}.
\end{itemize}
Im einfachsten Fall werden die Akzeptanzgrenzen und die Toleranzgrenzen gleichgesetzt. Es werden also Messwerte akzeptiert, die auf der Toleranzgrenze sind. Der (wahre) Wert findet sich somit mit 50\%-iger Wahrscheinlichkeit innerhalb, aber auch genauso
wahrscheinlich außerhalb des zulässigen Toleranzbereichs.
Da in einem solchen Fall eine fifty-fifty-Chance besteht, die
richtige oder falsche Konformitätsentscheidung zu treffen,
wird dieses Verfahren als das \textbf{shared-risk}-Verfahren
bezeichnet (geteiltes Risiko).

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[width=90mm]{05_vorlesung/media/SharedRisk.png}
		\caption{\label{fig:shared_risk} Beispiel für \textsl{shared risk}: Die Akzeptanzgrenze ist gleich der Toleranzgrenze und der Messwert liegt auf dieser Grenze.}
	\end{center}
\end{figure}

Um das Risiko umzuverteilen wird ein Sicherheitsband zwischen Akzeptanz- und Toleranzgrenze eingeführt. Die Länge des Sicherheitsbandes bestimmt das Risiko. Man unterscheidet hier zwischen \textbf{überwachte Annahme} (\textsl{guarded acceptance})
und \textbf{überwachte Ablehnung} (\textsl{guarded rejection}).
Bei der überwachten Annahme wird das Risiko minimiert, fälschlicherweise ein nicht konformes Produkt für gut zu befinden. Hierzu wird die Akzeptanzgrenze tief in das Toleranzintervall hineingelegt, so dass nur solche Messwerte akzeptiert werden, deren zugehörige Unsicherheitsverteilung darauf schließen lassen, den (wahren) Wert mit großer Wahrscheinlichkeit innerhalb des Toleranzbandes zu finden. Der Sicherheitsabstand $w= 	T_\mathrm{U} - A_\mathrm{U}$ ist größer als Null. Die überwachte Annahme spielt bspw. bei Produkten, die man in den Verkehr bringen will, eine Rolle. Man will, dass die Produkte konform mit den Spezifikationen sind.

\begin{figure}[!htp]
	\begin{center}
	\includegraphics[width=160mm]{05_vorlesung/media/AnnahmeRueckweisungMitToleranz.png}
		\caption{\label{fig:Verteilung_Risiko}} Beispiele für überwachte Annahme (links) und überwachte Rückweisung / Ablehnung (rechts).
	\end{center}
\end{figure}

Bei der überwachten Ablehnung wird das Risiko, dass man zu Unrecht eine Übertretung des Toleranzbandes festgestellt hat, minimiert.
Der Sicherheitsabstand $w= 	T_\mathrm{U} - A_\mathrm{U}$ ist kleiner als Null.
Dies findet bspw. Anwendung bei der Geschwindigkeitsüberwachung im Straßenverkehr und
entspricht der juristischen Entscheidungsregel \glqq in dubio pro reo\grqq ~(lat.\ für \glqq im Zweifel für
den Angeklagten\grqq).

Mit Hilfe der Konformitätswahrscheinlichkeiten $p_\mathrm{c}$ können das \textbf{spezifische Konsumentenrisiko} und das \textbf{spezifische Produzentenrisiko} berechnet werden. Unter dem spezifischen Konsumentenrisiko versteht man die Wahrscheinlichkeit, dass ein einzelner akzeptierter Gegenstand oder ein Produkt angenommen / akzeptiert wird, obwohl er/es nicht konform ist, d.h. nicht innerhalb der Toleranzgrenzen liegt (siehe oben, Fall (b)). Das spezifische Konsumentenrisiko berechnet sich zu $1-p_\mathrm{c}$. Das spezifische Produzentenrisiko ist definiert als die Wahrscheinlichkeit, dass ein einzelner abgelehnter Gegenstand oder ein Produkt konform gewesen wäre. Die Wahrscheinlichkeit, dass ein abgelehnter Gegenstand/Produkt doch konform gewesen wäre, entspricht der Konformitätswahrscheinlichkeit $p_\mathrm{c}$.
Die beiden spezifischen Risiken hängen u.a.\ vom Messwert und Sicherheitsabstand ab. Die
Risiken sind maximal, wenn der Messwert $\bar y = A_\textrm{U}$ ist. Ein positiver Sicherheitsabstand beeinflusst die beiden Risiken in komplementärer Weise. Ein positiver Sicherheitsabstand verkleinert das Konsumentenrisiko und erhöht das Produzentenrisiko.
Ein negativer Sicherheitsabstand verkleinert das Produzentenrisiko und erhöht das Konsumentenrisiko. Für den Fall  \glqq kein Sicherheitsabstand\grqq ~haben wir wieder
den Grenzfall \glqq geteiltes Risiko\grqq.

Neben den spezifischen Risiken werden im \cite{JCGM106} auch die globalen Risiken definiert. Sie geben bspw. die mittlere Wahrscheinlichkeit der Fehleinschätzung basierend auf einem Messprozess an oder die mittlere Wahrscheinlichkeit einer Fehlbewertung über eine große Anzahl durchgeführter Messungen oder die mittlere Wahrscheinlichkeit der Fehlbewertung durch eine zukünftige Messung an. Das globale Risiko ist wichtig für die Planung und hat globale Auswirkungen. Man unterscheidet auch hier zwischen globalem Konsumentenrisiko und globalem Produzentenrisiko.
Gegeben sei das Priorwissen $p_0(y)$ über die Messgröße $Y$ sowie die durchgeführte Messung mit den Messwerten $X_1,\ldots , X_J$. Die durchgeführte Messung wird durch die Wahrscheinlichkeitsdichtefunktion $p(x|y)$ beschrieben. Mit Hilfe des Bayes-Theorems ergibt sich die PDF der Posterior proportional zu $p_0(y)\cdot p(x|y)$. Für das \textbf{globale Konsumentenrisiko} $R_\mathrm{C}$  wird über das Akzeptanzintervall $A$ und über den
Bereich außerhalb des Toleranzintervalls $\tilde C$ integriert:
\begin{equation}
	R_\mathrm{C} =  \int_{\tilde C} \int_{A} \; p_0(y) \cdot p(x|y) \;\mathrm{d}x\; \mathrm{d}y.
	\label{eq:globalesKonsumentenrisiko}
\end{equation}

Für das \textbf{globale Produzentenrisiko} $R_\mathrm{P}$ wird über den Bereich außerhalb des Akzeptanzintervalls $\tilde A$ und über das Toleranzintervall $C$ integriert

\begin{equation}
	R_\mathrm{P} =  \int_{C} \int_{\tilde A} \; p_0(y) \cdot p(x|y) \; \mathrm{d}x\; \mathrm{d}y.
		\label{eq:globalesProduzentenrisiko}
\end{equation}

Ein aus der Richtlinie zur Konformitätsprüfung \cite{JCGM106} entnommenes Beispiel
zeigt sehr anschaulich, wie durch eine dem Produktionsprozess nachgeschaltete
Qualitätsprüfung das Risiko signifikant minimiert, Ausschuss auszuliefern.

Es sollen Widerstände mit einem nominalen Widerstandswert $y_0 = 1500~\Omega$ produziert werden (Vorgabe/ Zielwert). Die Toleranzgrenzen sind wie folgt spezifiziert:
\[
T_\mathrm{L} = 1499.80~\Omega \quad \textrm{und} \quad T_\mathrm{U} = 1500.20~\Omega
\]

Der Erwartungswert des Widerstands sowie die Streuung der Widerstandswerte
durch den Herstellungsprozess sind dem  Menschen nicht bekannt,
aber wir betrachten dies für die Abschätzung der
Risiken, als wüssten wir diese. Wir verwenden als \textsl{wahren} Erwartungswert
den Wert der Zielvorgabe ein und als \textsl{wahre}
Streuung den Wert
\[
\sigma_0 = 0.12~\Omega ,
\]
der daraus gewonnen wurde, dass sich der Hersteller bei Einführung der Produktionslinie
ein ultrapräzises Ohmmeter ausgeliehen hatte, dessen Unsicherheit kleiner ist als
$0.001~\Omega$.
Die Wahrscheinlichkeitsdichteverteilung der produzierten Widerstandswerte
ist der Prior $p_0(y)$ mit
\begin{equation}
p_0(y) \; = \; \frac{1}{\sqrt{2 \pi} \; \sigma_0}
	e^{-\frac{1}{2}\left(\frac{y - y_0}{\sigma_0}\right)^2} \; = \;
 \frac{1}{\sqrt{2 \pi} \cdot 0.12}
	e^{-\frac{1}{2}\left(\frac{y - 1500}{0.12}\right)^2}.
\end{equation}


Wegen der Unsicherheit des Messvorgangs mit dem herstellereigenen Ohmmeter, das er
in seiner Qualitätsprüfung, also bei der Inspektion, einsetzt, ist nicht sichergestellt,
dass wenn ein Widerstand als außerhalb der
Fertigungstoleranz liegend, also außerhalb der Intervalls $[T_\mathrm{L}, T_\mathrm{U}]$
gemessen wird, dass er wirklich außerhalb liegt.
Das in diesem Beispiel in der Qualitätsprüfung verwendete Ohmmeter hat eine
Unsicherheit $u$ von
 \[
\sigma = 0.04~\Omega, \; \mathrm{d.h.}\quad u= 0.04~\Omega \quad \mathrm{bzw.} \quad U(k=2) = 0.08~\Omega
\]
Es kann zum Nachteil des Käufers eines Widerstands passieren,
dass der \textsl{wahre} Wert $x_1$ eines einzelnen Bauteils außerhalb der
Toleranzgrenzen liegt, dass aber das Ohmmeter der Qualitätsprüfung einen Wert misst, der
innerhalb des Akzeptanzintervalls liegt. Abb.~\ref{risikoQS}, \textsl{links}, zeigt ein Beispiel, bei dem der \textsl{wahre} Widerstandswert $y = x_1 = 1500.23 \, \Omega$ beträgt.
Die Wahrscheinlichkeitsdichteverteilung für die Werte, die das Ohmmeter für das
Bauteil mit $x_1 = 1500.23 \, \Omega$ anzeigt, ist
\begin{equation}
p(x | y=x_1) \; = \;  \frac{1}{\sqrt{2 \pi} \; \sigma}
	e^{-\frac{1}{2}\left(\frac{x - x_1}{\sigma}\right)^2} \; = \;
 \frac{1}{\sqrt{2 \pi} \cdot 0.04}
	e^{-\frac{1}{2}\left(\frac{x - 1500.23}{0.04}\right)^2}.
\end{equation}
Das Produkt
\begin{equation}
p(x | y=x_1) \cdot p_0(y=x_1) \; = \; \frac{1}{\sqrt{2 \pi} \; \sigma}
	e^{-\frac{1}{2}\left(\frac{x - x_1}{\sigma}\right)^2}
  \frac{1}{\sqrt{2 \pi} \; \sigma_0} \;
  	e^{-\frac{1}{2}\left(\frac{x_1 - y_0}{\sigma_0}\right)^2}
\end{equation}
mit Einsetzen der Zahlenwerte also
\begin{equation*}
p(x | y=x_1) \cdot p_0(y=x_1)  \; = \;
 \frac{1}{\sqrt{2 \pi} \cdot 0.04}
	e^{-\frac{1}{2}\left(\frac{x - 1500.23}{0.04}\right)^2}
  \frac{1}{\sqrt{2 \pi} \cdot 0.12}
   e^{-\frac{1}{2}\left(\frac{1500.23 - 1500}{0.12}\right)^2}
\end{equation*}
ist als blaue Kurve dargestellt, während der Prior als schwarze Kurve in
dem Diagramm in Abb.~\ref{risikoQS}, \textsl{links}, eingezeichnet ist.
\begin{figure}
\begin{center}
\includegraphics[width=80mm]{05_vorlesung/media/Konsumentenrisiko_x0p03.pdf}
\includegraphics[width=80mm]{05_vorlesung/media/Produzentenrisiko_x0p03.pdf}
\caption{Veranschaulichung zur Berechnung der globalen Risiken, \textsl{links:}
des Konsumenten bzw.\ Käufers, (\textsl{rechts}) des Produzenten bzw.\
Herstellers}
\label{risikoQS}
\end{center}
\end{figure}

Es kann zum Nachteil des Herstellers passieren, dass der \textsl{wahre} Wert
eines einzelnen Bauteils $x_1$ innerhalb der
Toleranzgrenzen - sogar innerhalb des Akzeptanzintervalls - liegt,
dass aber das Ohmmeter der Qualitätsprüfung einen Wert misst, der
außerhalb des Akzeptanzintervalls liegt. Abb.~\ref{risikoQS}, \textsl{rechts}, zeigt ein Beispiel, bei dem der \textsl{wahre} Widerstandswert $y = x_1 = 1500.15 \, \Omega$ beträgt.
Hier sind das Produkt
\begin{equation*}
p(x | y=x_1) \cdot p_0(y=x_1) \; = \;
 \frac{1}{\sqrt{2 \pi} \cdot 0.04}
	e^{-\frac{1}{2}\left(\frac{x - 1500.15}{0.04}\right)^2}
  \frac{1}{\sqrt{2 \pi} \cdot 0.12}
   e^{-\frac{1}{2}\left(\frac{1500.15 - 1500}{0.12}\right)^2}
\end{equation*}
wieder als blaue Kurve und der Prior als schwarze Kurve zu sehen.

Die roten Flächen unter den Kurven der beiden Diagramme in Abb.~\ref{risikoQS}
zeigen jeweils die Wahrscheinlichkeit dafür, dass im Falle \textsl{links} mit
$y = x_1 = 1500.23 \, \Omega$ der schlechte Widerstand mit einen Wert innerhalb des
Akzeptanzintervalls gemessen wird, folglich als gut verkauft wird, und dass im Falle \textsl{rechts} mit
$y = x_1 = 1500.15 \, \Omega$ der gute Widerstand mit einen Wert außerhalb des
Akzeptanzintervalls gemessen wird, folglich als schlecht verworfen wird.

Berechnet man für alle Werte $x_1$, die außerhalb des Toleranzintervalls die roten
Flächen der Teile solcher Produktkurven $p(x | y=x_1) \cdot p_0(y=x_1)$ d.h.\ integriert
man $x$ über diesen Bereich wie im linken Diagramm,
 um die Wahrscheinlichkeiten, dass schlechte Widerstände als gut
gemessen und somit verkauft werden, zu bestimmen und integriert dann noch
über alle diese Werte $x_1$, so erhält man die Wahrscheinlichkeit des Risikos, dass
der Käufer / Konsument  trotz Qualitätsprüfung Ausschussware erhält. Diese
Wahrscheinlichkeit heißt \textbf{globales Konsumentenrisiko}.

Berechnet man für alle Werte $x_1$, die innerhalb des Toleranzintervalls die roten
Flächen der Teile solcher Produktkurven $p(x | y=x_1) \cdot p_0(y=x_1)$ d.h.\ integriert
man $x$ über diesen Bereich wie im rechten Diagramm,
 um die Wahrscheinlichkeiten, dass gute Widerstände als schlecht
gemessen und somit als Ausschuss verworfen werden, zu bestimmen und integriert dann noch
über alle diese Werte $x_1$, so erhält man die Wahrscheinlichkeit des Herstellers
trotz Qualitätsprüfung gute Ware vernichtet. Diese
Wahrscheinlichkeit heißt \textbf{globales Produzentenrisiko}.

Dieses Beispiel zeigt, dass der Aufwand lohnt, in einen Qualitätssicherungsprozess zu investieren.

Die Konformitätswahrscheinlichkeit $p_\mathrm{c}$, Gl.~(\ref{eq:Konformitaetswahrscheinlichkeit}), liegt in diesem Beispiel bei
\[
	p_\mathrm{c} = \frac{1}{\sqrt{2 \pi} s} \int\limits_{T_\mathrm{L}}^{T_\mathrm{U}}
e^{-\frac{1}{2}\left(\frac{y - \bar y}{s}\right)^2} \mathrm{d}y  =
\frac{1}{\sqrt{2 \pi} \cdot 0.12} \int\limits_{T_\mathrm{L}=1499.80}^{T_\mathrm{U}=1500.20}
e^{-\frac{1}{2}\left(\frac{y - 1500}{0.12}\right)^2} \mathrm{d}y  \approx 0.905
\]
was bedeutet, dass im Rahmen des Produktionsprozesses etwa 90\% der Widerstände konform sind,
jedoch 10\% der Widerstände, die auf den Markt gebracht würden, nicht konform wären. Das Konsumentenrisiko
liegt somit bei 10\% Ausschussware.

Die Akzeptanzgrenzen werden innerhalb des Toleranzintervalls gewählt, um das
Konsumentenrisiko zu reduzieren:
\[
A_\mathrm{L} = 1499.82~\Omega \quad \textrm{und} \quad A_\mathrm{U} = 1500.18~\Omega
\]
Das Sicherheitsband beträgt $w = (1500.20-1500.18)~\Omega = 0.02~\Omega = 0.25 U$

Im Vergleich zu den $10 \, \%$ Ausschuss liefert
\begin{itemize}
\item das globale Konsumentenrisiko, Gl.(\ref{eq:globalesKonsumentenrisiko}),
\begin{equation}
	R_\mathrm{C} =  \int_{\tilde C} \int_{A} \; p_0(y) \; p(x|y) \;\mathrm{d}x\; \mathrm{d}y
	\label{eq:globalesKonsumentenrisiko_Aufg}
\end{equation}
d.h.
$$
R_\mathrm{C} =  \int\limits_{-\infty}^{T_\mathrm{L}} \, \mathrm{d}y\; p_0(y) \;
 \int\limits_{A_\mathrm{L}}^{A_\mathrm{U}} \mathrm{d}x \; p(x|y) \; + \;
 \int\limits_{T_\mathrm{U}}^{\infty} \, \mathrm{d}y\; p_0(y) \;
  \int\limits_{A_\mathrm{L}}^{A_\mathrm{U}} \mathrm{d}x \; p(x|y)
$$
mit
\begin{equation}
	p(x|y) =\frac{1}{\sqrt{2 \pi} \sigma}
	e^{-\frac{1}{2}\left(\frac{x - y}{\sigma}\right)^2} =
	\frac{1}{\sqrt{2 \pi} \cdot 0.04}
		e^{-\frac{1}{2}\left(\frac{x - y}{0.04}\right)^2}
\end{equation}
und nach Einsetzen der Zahlenwerte
$$
 R_\mathrm{C} = \int\limits_{-\infty}^{T_\mathrm{L}} \, \mathrm{d}y\;
 \frac{1}{\sqrt{2 \pi} \cdot 0.12}
 e^{-\frac{1}{2}\left(\frac{y - 1500}{0.12}\right)^2}
 \int\limits_{A_\mathrm{L}}^{A_\mathrm{U}} \mathrm{d}x \;
 \frac{1}{\sqrt{2 \pi} \cdot 0.04}
   e^{-\frac{1}{2}\left(\frac{x - y}{0.04}\right)^2}
$$
$$
   \; + \;
   \int\limits_{T_\mathrm{U}}^{\infty} \, \mathrm{d}y\;
  \frac{1}{\sqrt{2 \pi} \cdot 0.12}
  e^{-\frac{1}{2}\left(\frac{y - 1500}{0.12}\right)^2}
  \int\limits_{A_\mathrm{L}}^{A_\mathrm{U}} \mathrm{d}x \;
  \frac{1}{\sqrt{2 \pi} \cdot 0.04}
    e^{-\frac{1}{2}\left(\frac{x - y}{0.04}\right)^2}
  \; \approx \; 1~\%
$$
\item Das globale Produzentenrisiko berechnet sich mit Gl.(\ref{eq:globalesProduzentenrisiko}) zu
\begin{equation}
	R_\mathrm{P} =  \int_{C} \int_{\tilde A} \; p_0(y) \; p(x|y) \; \mathrm{d}x\; \mathrm{d}y
  \label{eq:globalesProduzentenrisiko_Aufg}
\end{equation}
$$
	=  \int\limits_{T_\mathrm{L}}^{T_\mathrm{U}} \, \mathrm{d}y\; p_0(y) \;
 \left(\int\limits_{-\infty}^{A_\mathrm{L}} \mathrm{d}x \; p(x|y)
 + \int\limits_{A_\mathrm{U}}^{\infty} \mathrm{d}x \; p(x|y) \right)  \approx 7~\%
$$
\end{itemize}
Die beiden Integrale Gl.~(\ref{eq:globalesKonsumentenrisiko_Aufg})
und Gl.~(\ref{eq:globalesProduzentenrisiko_Aufg}) werden numerisch gelöst,
siehe auch \cite{JCGM106}.

Durch die Qualitätsprüfung wurde das Risiko des Käufers,
Ausschussware geliefert zu bekommen, von 10\% auf 1\% reduziert, und damit das
Problem von Reklamation und Regressansprüchen signifikant reduziert.

Von 100 produzierten Widerständen werden im Produktionsprozess 90 als konform und 10 als nicht konform deklariert. Im anschließenden Inspektionsprozess werden dann von den 90 konformen Widerständen 83 akzeptiert und 7 fälschlicherweise als nicht konform ausgemustert. Von den 10 nicht konformen Widerständen werden im Inspektionsprozess 9 Widerstände verworfen und 1 Widerstand fälschlicherweise akzeptiert.
Insgesamt werden also nach dem Inspektionsprozess 84 Widerstände akzeptiert und
auf den Markt gebracht. 83 Widerstände sind konform, 1 Widerstand ist nichtkonform., d.h. $83/84 \approx 99~\%$ sind konform.
Durch diese Maßnahme wurde das Konsumentenrisko reduziert, jedoch gelangen eben nur noch 84 von 100 Widerständen auf den Markt. Umgekehrt werden auch 7 konforme Widerstände ausgesondert. An dem Beispiel sieht man eben sehr schön,
dass es schlussendlich ein Abwägen ist zwischen niedrigem Konsumentenrisiko und
Anzahl der Widerstände, die auf den Markt gebracht werden können.

Matlab/Octave-Code zur numerischen Berechnung des Konsumenten- und Produktionsrisikos:

\begin{lstlisting}[style=Matlab]
function konform_example_R()
step = 0.0002;
perc = '%';

gaussian = @(x, mu, sigma) exp(-0.5*((x - mu) / sigma).^2) / (sigma * sqrt(2*pi));
% ----
y0 = 1500;
s0 = 0.12;
T_L = 1499.80;
T_U = 1500.20;
A_L = 1499.82;
A_U = 1500.18;
mininf = T_L - 3*s0;
maxinf = T_U + 3*s0;

y = [A_L:step:A_U];
prior = step * gaussian(y, y0, s0);
p_c = sum(prior);
printf('spezifisches Konsumentenrisiko bzgl A ..................: %3.0f %c\n', 100 - 100*p_c, perc);


y = [T_L:step:T_U];
prior = step * gaussian(y, y0, s0);
p_c = sum(prior);
printf('spezifisches Konsumentenrisiko bzgl T Gl (21) in JCGM106: %3.0f %c\n', 100 - 100*p_c, perc);

% inspection device - instrument uncertainty
s_insp = 0.04;

yall = [mininf:step:maxinf];
resistor_distri = exp(-0.5*((yall - y0) / s0).^2) / (s0 * sqrt(2*pi));

% ------
% global producer risk:
x_L = [mininf:step:A_L]'; %'
x_U = [A_U:step:maxinf]'; %'

n_y = length(y);
n_xL = length(x_L);
n_xU = length(x_U);

px_giveny_L = step * gaussian(x_L * ones(1,n_y), ones(n_xL,1) * y, s_insp);
px_giveny_U = step * gaussian(x_U * ones(1,n_y), ones(n_xU,1) * y, s_insp);

posterior_L = (ones(n_xL,1)*prior) .* px_giveny_L;
posterior_U = (ones(n_xU,1)*prior) .* px_giveny_U;
R_p = sum(posterior_L(:)) + sum(posterior_U(:));
printf('globales Produzentenrisiko: %3.0f %c\n', 100*R_p, perc);

%-------
% global consumer risk

x = [A_L:step:A_U]'; %'
y_L = [mininf:step:T_L];
y_U = [T_U:step:maxinf];

n_x = length(x);
n_yL = length(y_L);
n_yU = length(y_U);
prior_L = step * ones(n_x,1) * gaussian(y_L, y0, s0);
prior_U = step * ones(n_x,1) * gaussian(y_U, y0, s0);
px_giveny_L = step * gaussian(x * ones(1,n_yL), ones(n_x,1) * y_L, s_insp);
px_giveny_U = step * gaussian(x * ones(1,n_yU), ones(n_x,1) * y_U, s_insp);
posterior_L = prior_L .* px_giveny_L;
posterior_U = prior_U .* px_giveny_U;
R_c = sum(posterior_L(:)) + sum(posterior_U(:));
printf('globales Konsumentenrisiko: %3.0f %c\n', 100*R_c, perc);
end

\end{lstlisting}

%\bibliography{$BIBLIO/math}
%\bibliographystyle{unsrt}
%\bibliography{$BIBLIO/math}
%\bibliographystyle{unsrt}

\section{Übungsaufgaben zum Selbststudium}
\label{AufgVorl5}

\textbf{Aufgabe 1}\\

Probieren Sie anhand von Beobachtungen, die Sie mit Hilfe eines Generators, der
normalverteilte Zufallszahlen liefert, den Kolmogorow-Smirnow-Test, kurz KS-Test, aus.

In Matlab/Gnu-Octave könnten Sie dies beispielsweise wie folgt realisieren:

\begin{lstlisting}[style=Matlab]
  J1 = 2000; % Stichprobenumfang
  mu1 = 23; % Erwartungswert der Zufallsgroesse
  s1 = 3;   % Wurzel aus dem Erwartungswert fuer die Varianz
  Xarray = s1*randn(J1,1) + mu1;
\end{lstlisting}
Das Sortieren mit Matlab/Gnu-Octave können Sie mit
\begin{lstlisting}[style=Matlab]
  [xsort, isort] = sort(Xarray, 'ascend');
\end{lstlisting}
und die Wahrscheinlichkeiten mit
\begin{lstlisting}[style=Matlab]
  h = [1:J1]'/J1;
\end{lstlisting}
berechnen.

\begin{enumerate}
\item Verwenden Sie nicht die {\`a} priori eingesetzten Erwartungswerte,
 sondern Mittelwert \texttt{y = mean(Xarray)} und empirische Standardabweichung
 \texttt{s = std(Xarray)}, um die Funktion $P(X)$ aus Gl.~(\ref{cdfKS})
 zu realisieren. Berechnen Sie die relativen Häufigkeiten $H$ gemäß
 Gl.~(\ref{cdfH}), sowie den in Gl.~(\ref{KSpruefgroesse}) definierten
 Schwellwert
  $$K_{\alpha, J}$$
 zu einem Signifikanzniveau von $\alpha = 0.05$,
 wobei $J$ der Stichprobenumfang ist.
 Führen Sie den Test mit den von Ihnen erzeugten Zufallswerten durch.
\item Erzeugen Sie einen zusätzlichen Satz von Zufallszahlen, beispielsweise mit den
 Werten \texttt{mu2 = 35} und \texttt{s2 = 5}. Wählen Sie für diese einen
 deutlich kleineren Stichprobenumfang. Führen Sie den KS-Test für die
 vereinigten Zufallszahlenarrays durch und tun Sie so, als ob Sie nicht
 wüssten, dass dies keine gemeinsame Grundgesamtheit ist.
\end{enumerate}



\textbf{Aufgabe 2}

Gegeben seien zwei \textsl{gepoolte} Stichproben

Stichprobe 1:

\begin{tabular}{|c|c|c|c|c|}
\hline
21 & 33 & 19 & 39 & 7\\
\hline
\end{tabular}

Stichprobe 2:

\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
53 & 69 & 63 & 47 & 49 & 44 & 47 & 44 & 38\\
\hline
\end{tabular}

\begin{enumerate}
\item[a)] Geben Sie zu jeder der beiden Stichproben die Mittelwerte und Standardabweichungen an.
\item[b)] Prüfen Sie auf einem Signifikanzniveau von $\alpha = 0.05$ die Hypothese $H_0$,
 ob beide Stichproben zu einer Grundgesamtheit
 mit demselben Erwartungswert $\mu$ gehören. Geben Sie dazu die Formel und den Wert
 der Testgröße an und vergleichen Sie diese mit dem entsprechenden Quantil der für diesen
 Test zu verwendenden Verteilung. Mit welcher Verteilung ist dieser Test durchzuführen?
\item[c)] Prüfen Sie auf einem Signifikanzniveau von $\alpha = 0.05$ die Hypothese $H_0$,
 ob die zweite Stichprobe zu einer Grundgesamtheit
 mit der Standardabweichung $\sigma_0 = 6$ gehört. Geben Sie dazu die Formel und den Wert
 der Testgröße an und vergleichen Sie diese mit dem entsprechenden Quantil der für diesen
 Test zu verwendenden Verteilung. Mit welcher Verteilung ist dieser Test durchzuführen?
 Führen Sie nur den einseitigen Test durch, das heißt prüfen Sie nur bezüglich
 des rechten \textsl{Tail} der Verteilungsdichte.
\item[d)] Führen Sie denselben Test wie in (c) durch, aber dieses Mal für $\sigma_0 = 9$.
\end{enumerate}

Verwenden Sie die Quantiltabellen aus dem Anhang Kapitel~\ref{quantiltabellen}.
